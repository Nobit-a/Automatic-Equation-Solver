{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Maths.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZQXQh4cnRYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import re\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNTOnmbnnqKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "655ff445-4868-40f8-a7e3-3b6f18db0d1a"
      },
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dcZ73ZInxlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cd5f267-8957-4902-c346-402ea999a93e"
      },
      "source": [
        "%cd 'drive/My Drive/Colab/Maths'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab/Maths\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kH0DqwRo3EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('train.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    train_data = list(reader)\n",
        "\n",
        "with open('test.csv', newline='') as f:\n",
        "    reader = csv.reader(f)\n",
        "    test_data = list(reader)\n",
        "\n",
        "train_data = train_data[1:]\n",
        "test_data = test_data[1:]\n",
        "test_data = [row[0] for row in test_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alEd0q-_pGXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a7159de-6169-430a-c779-1ea056b3e6f8"
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['add 45 and 71', '45', '71', 'x+y', '116']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQXVUAlOqRKU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating question/answer pairs with question being input statement and answer as n1 n2 equation\n",
        "qa_pairs = []\n",
        "for row in train_data:\n",
        "    ques = row[0]\n",
        "    ans = str(row[1]) + \" \" + str(row[2]) + \" \" + str(row[3])\n",
        "    qa_pairs.append([ques, ans])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NIWHWiSrkYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72a5fe2c-7386-4e52-9ad1-c76b15640a7d"
      },
      "source": [
        "qa_pairs[0]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['add 45 and 71', '45 71 x+y']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-jhTgk7r1Qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word Processing and Vocabulary\n",
        "\n",
        "PAD_TOKEN = 0\n",
        "SOS_TOKEN = 1\n",
        "EOS_TOKEN = 2\n",
        "\n",
        "class Vocabulary():\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_TOKEN: \"PAD\", SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
        "        self.numwords = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.numwords\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.numwords] = word\n",
        "            self.numwords += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fnen1actbUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c198f547-2161-41ee-aaec-2b8a77d5d452"
      },
      "source": [
        "# Add the question/answer pairs to the vocabulary\n",
        "\n",
        "vocab = Vocabulary()\n",
        "\n",
        "for pair in qa_pairs:\n",
        "    vocab.addSentence(pair[0])\n",
        "    vocab.addSentence(pair[1])\n",
        "\n",
        "print(\"Unique word count = {}\".format(vocab.numwords))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique word count = 120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPzFCKn1uGqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode a sentence into a list of integers\n",
        "\n",
        "def indexesFromSentence(vocab, sentence):\n",
        "    return [vocab.word2index[word] for word in sentence.split(' ')] + [EOS_TOKEN]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTyT1sFjvC_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e576e055-b889-4bcc-87e6-d88a1870d1dd"
      },
      "source": [
        "print(qa_pairs[1][0])\n",
        "indexesFromSentence(vocab, qa_pairs[1][0])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "add 53 and 34\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 8, 5, 9, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUFvLFcOvjcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Padding the sentences for equal length\n",
        "def zeroPadding(l, fillvalue = 0):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duN1sSafuseG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8c92a698-b2e9-46a0-87af-4718da99fa69"
      },
      "source": [
        "# Defining a sample for testing\n",
        "inp = []\n",
        "out = []\n",
        "for pair in qa_pairs[:10]:\n",
        "    inp.append(pair[0])\n",
        "    out.append(pair[1])\n",
        "indexes = [indexesFromSentence(vocab, sentence) for sentence in inp]\n",
        "\n",
        "print(inp)\n",
        "print(indexes)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['add 45 and 71', 'add 53 and 34', 'add 22 and 35', 'add 38 and 97', 'add 87 and 71', 'add 31 and 81', 'add 40 and 97', 'add 8 and 40', 'add 28 and 49', 'add 27 and 26']\n",
            "[[3, 4, 5, 6, 2], [3, 8, 5, 9, 2], [3, 10, 5, 11, 2], [3, 12, 5, 13, 2], [3, 14, 5, 6, 2], [3, 15, 5, 16, 2], [3, 17, 5, 13, 2], [3, 18, 5, 17, 2], [3, 19, 5, 20, 2], [3, 21, 5, 22, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vedvy5dEvrrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2f3c9e43-8a43-483b-db1b-5ca35b5c0107"
      },
      "source": [
        "# Each sentence is read downwards\n",
        "test_result = zeroPadding(indexes)\n",
        "test_result"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 3, 3, 3, 3, 3, 3, 3, 3, 3),\n",
              " (4, 8, 10, 12, 14, 15, 17, 18, 19, 21),\n",
              " (5, 5, 5, 5, 5, 5, 5, 5, 5, 5),\n",
              " (6, 9, 11, 13, 6, 16, 13, 17, 20, 22),\n",
              " (2, 2, 2, 2, 2, 2, 2, 2, 2, 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8PFZaAtwwKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To create a mask later\n",
        "def binaryMatrix(l, value=0):\n",
        "    m = []\n",
        "    for i,seq in enumerate(l): # l is a list of lists just like above\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_TOKEN:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CABjV_gkx6Xi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "85891282-18e8-4012-e155-4e3ece564da9"
      },
      "source": [
        "binaryResult = binaryMatrix(test_result)\n",
        "binaryResult"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSqeUb7ix7Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Functions to make entire data suitable for the net \n",
        "# now that the mini functions are tested\n",
        "\n",
        "def inputVar(l, vocab):\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "def outputVar(l, vocab):\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeUa0h7Eylga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch2TrainData(vocab, pair_batch):\n",
        "    # Sort by QUESTION LENGTH in DESCENDING order\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(' ')), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, vocab)\n",
        "    out, mask, max_target_len = outputVar(output_batch, vocab)\n",
        "    return inp, lengths, out, mask, max_target_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dknp0vDy4WF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "4ed5ebf8-e7c6-47c6-bcbb-26eabc6b8ea0"
      },
      "source": [
        "# Example \n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(vocab, [random.choice(qa_pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"Input Variable:\")\n",
        "print(input_variable)\n",
        "print(\"Lengths of each sentence:\")\n",
        "print(lengths)\n",
        "print(\"Target Variable:\")\n",
        "print(target_variable)\n",
        "print(\"Mask:\")\n",
        "print(mask)\n",
        "print(\"Max target length : \", max_target_len)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Variable:\n",
            "tensor([[105, 110, 105, 112,  42],\n",
            "        [ 84,  29,  54,  77, 116],\n",
            "        [106, 106, 106, 113, 117],\n",
            "        [ 19,  77,  95,  55,   2],\n",
            "        [  2,   2,   2,   2,   0]])\n",
            "Lengths of each sentence:\n",
            "tensor([5, 5, 5, 5, 4])\n",
            "Target Variable:\n",
            "tensor([[ 84,  29,  54,  77,  42],\n",
            "        [ 19,  77,  95,  55,  92],\n",
            "        [  7, 111,   7, 114, 118],\n",
            "        [  2,   2,   2,   2,   2]])\n",
            "Mask:\n",
            "tensor([[True, True, True, True, True],\n",
            "        [True, True, True, True, True],\n",
            "        [True, True, True, True, True],\n",
            "        [True, True, True, True, True]])\n",
            "Max target length :  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mpeWhzZy_UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################### DEFINING THE MODEL ##################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebPYh5KJz7Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "\n",
        "        embedded = self.embedding(input_seq)\n",
        "\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) \n",
        "\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "\n",
        "        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:,self.hidden_size:]\n",
        "\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBKvVHxt0Jzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output): # Hidden is the hidden state from the decoder\n",
        "            return torch.sum(hidden*encoder_output, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_output):\n",
        "\n",
        "        attention_energies = self.dot_score(hidden, encoder_output) # Max_length x batch_size\n",
        "        attention_energies = attention_energies.t() # Transpose\n",
        "\n",
        "        return F.softmax(attention_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdyMdFEq0PsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, attention_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention_model = attention_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attention = Attention(attention_model, hidden_size)\n",
        "    \n",
        "    def forward(self, input_step, last_hidden, encoder_output):\n",
        "        # Input step = (1, batch_size), cause one row of words (one batch) picked up from the array of sentence length x batch size\n",
        "        # Last hidden is the final hidden state of the encoder GRU (n_layers x directions, batch size, hidden size)\n",
        "        # encoder output is the output of the encoder(full memory) (sentence len, batch size, directions x hidden size)\n",
        "        # We run this one step (one batch of words) at a time\n",
        "        \n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden) # RNN_Output = (1, batch size, hidden size x directions)\n",
        "                                                             # Hidden state = (n_layers x directions, batch size, hidden size)\n",
        "\n",
        "        # Attention forward function returns softmax in the form (batch size, 1, max length)\n",
        "        attention_weights = self.attention(rnn_output, encoder_output)\n",
        "\n",
        "        # For the context vector, or what to focus on vector, we multiply the attention with the encoder output\n",
        "        # Attention (batch size, 1, max length) x Encoder output transpose (batch size, max length, hidden size) = (batch size, 1, hidden size)\n",
        "        context = attention_weights.bmm(encoder_output.transpose(0,1))\n",
        "\n",
        "        # Concatenate context with GRU output\n",
        "        rnn_output = rnn_output.squeeze(0) # Remove the 1 from that 3-D tensor to make it 2-D\n",
        "        context = context.squeeze(1) # Both of these are now batch size x hidden size 2-D tensors\n",
        "        concat_input = torch.cat((rnn_output, context),1) # Concatenate along columns, so new size = (batch size, hidden size x 2)\n",
        "        concat_output = torch.tanh(self.concat(concat_input)) # Pass the concat through a linear layer\n",
        "\n",
        "        output = self.out(concat_output) # Size now is batch size x vocab size\n",
        "        output = F.softmax(output, dim=1) # Each batch row contains the probabilities of all the words, so softmax across them to get \n",
        "                                          # the MOST PROBABLE WORD\n",
        "        return output, hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxuPtug-0S_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maskNLLLoss(decoder_out, target, mask): # To NOT calculate loss for padded spaces\n",
        "    nTotal = mask.sum() # Number of elements to consider\n",
        "    target = target.view(-1,1)\n",
        "\n",
        "    gathered_tensor = torch.gather(decoder_out, 1, target)\n",
        "\n",
        "    crossEntropy = -torch.log(gathered_tensor) # Calculate the loss on the gathered tensor\n",
        "\n",
        "    loss = crossEntropy.masked_select(mask)\n",
        "    loss = loss.mean()\n",
        "    loss = loss.to(device)\n",
        "\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRZ2QDwQ0Vvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################# TRAINING ########################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or4v7wQM0Z39",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4cd1ebb3-6c3f-47c1-9bd7-c58de047927a"
      },
      "source": [
        "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< This is only for visualization >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(vocab, [random.choice(qa_pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "### One time step is one batch of words ###\n",
        "\n",
        "print(\"Input Variable:\")\n",
        "print(input_variable)\n",
        "print(\"Lengths of each sentence:\")\n",
        "print(lengths)\n",
        "print(\"Target Variable:\")\n",
        "print(target_variable)\n",
        "print(\"Mask:\")\n",
        "print(mask)\n",
        "\n",
        "print(\"Input Variable Shape:\")\n",
        "print(input_variable.shape)\n",
        "print(\"Lengths Shape:\")\n",
        "print(lengths.shape)\n",
        "print(\"Target Variable Shape:\")\n",
        "print(target_variable.shape)\n",
        "print(\"Mask Shape:\")\n",
        "print(mask.shape)\n",
        "print(\"Max target length : \", max_target_len)\n",
        "\n",
        "# Defining the parameters\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "attention_model = 'dot'\n",
        "embedding = nn.Embedding(vocab.numwords, hidden_size)\n",
        "\n",
        "# Defining the encoder and decoder\n",
        "encoder = Encoder(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = Decoder(attention_model, embedding, hidden_size, vocab.numwords, decoder_n_layers,dropout)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001) # Parameters() specifies the weights of the encoder/decoder for the optimizer\n",
        "                                                               # to differentiate and subtract from and do whatever with\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\n",
        "encoder_optimizer.zero_grad()\n",
        "decoder_optimizer.zero_grad()\n",
        "\n",
        "input_variable = input_variable.to(device)\n",
        "lengths = lengths.to(device)\n",
        "target_variable = target_variable.to(device)\n",
        "mask = mask.to(device)\n",
        "\n",
        "loss = 0\n",
        "print_losses = []\n",
        "n_totals = 0\n",
        "\n",
        "encoder_output, encoder_hidden = encoder(input_variable, lengths)\n",
        "print(\"Encoder Output Shape = \",encoder_output.shape)\n",
        "print(\"Last Encoder Hidden State Shape = \",encoder_hidden.shape)\n",
        "\n",
        "decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(small_batch_size)]])\n",
        "decoder_input = decoder_input.to(device)\n",
        "print(\"Initial Decoder Input Shape = \",decoder_input.shape)\n",
        "print(decoder_input)\n",
        "\n",
        "# Last encoder hidden state is passed to the decoder as the initial hidden state\n",
        "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "print(\"Initial decoder hidden state shape = \",decoder_hidden.shape)\n",
        "print(\"\\n\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(\"THIS IS WHAT HAPPENS AT EVERY TIME STEP OF THE GRU!\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "for t in range(max_target_len):\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "    print(\"Decoder Output Shape = \", decoder_output.shape)    \n",
        "    print(\"Decoder Hidden State Shape = \", decoder_hidden.shape)\n",
        "\n",
        "    decoder_input = target_variable[t].view(1,-1) # Cause Teacher Forcing\n",
        "    print(\"Target Variable now = \", target_variable[t])\n",
        "    print(\"Target Variable Shape now = \", target_variable[t].shape)\n",
        "    print(\"Decoder input shape after reshaping = \", decoder_input.shape)\n",
        "\n",
        "    # Loss\n",
        "    print(\"Mask for current timestep\", mask[t])\n",
        "    print(\"Mask shape for current timestep\", mask[t].shape)\n",
        "    mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "    print(\"Mask Loss = \", mask_loss)\n",
        "    print(\"Total = \", nTotal)\n",
        "\n",
        "    loss += mask_loss\n",
        "    print_losses.append(mask_loss.item()*nTotal)\n",
        "    print(print_losses)\n",
        "    n_totals += nTotal\n",
        "    print(nTotal)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    returned_loss = sum(print_losses)/n_totals\n",
        "    print(\"Returned Loss = \", returned_loss)\n",
        "    print(\"\\n\")\n",
        "    print(\"----------------------------------------DONE ONE STEP-----------------------------------\")\n",
        "    print(\"\\n\")\n",
        " "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Variable:\n",
            "tensor([[105, 107,  15,  66,  70],\n",
            "        [ 69,  70, 116, 116, 116],\n",
            "        [106, 108, 117, 119, 119],\n",
            "        [ 80,  12,   2,   2,   2],\n",
            "        [  2,   2,   0,   0,   0]])\n",
            "Lengths of each sentence:\n",
            "tensor([5, 5, 4, 4, 4])\n",
            "Target Variable:\n",
            "tensor([[ 69,  70,  15,  66,  70],\n",
            "        [ 80,  12,  92,  71,  71],\n",
            "        [  7, 109, 118, 118, 118],\n",
            "        [  2,   2,   2,   2,   2]])\n",
            "Mask:\n",
            "tensor([[True, True, True, True, True],\n",
            "        [True, True, True, True, True],\n",
            "        [True, True, True, True, True],\n",
            "        [True, True, True, True, True]])\n",
            "Input Variable Shape:\n",
            "torch.Size([5, 5])\n",
            "Lengths Shape:\n",
            "torch.Size([5])\n",
            "Target Variable Shape:\n",
            "torch.Size([4, 5])\n",
            "Mask Shape:\n",
            "torch.Size([4, 5])\n",
            "Max target length :  4\n",
            "Encoder Output Shape =  torch.Size([5, 5, 500])\n",
            "Last Encoder Hidden State Shape =  torch.Size([4, 5, 500])\n",
            "Initial Decoder Input Shape =  torch.Size([1, 5])\n",
            "tensor([[1, 1, 1, 1, 1]], device='cuda:0')\n",
            "Initial decoder hidden state shape =  torch.Size([2, 5, 500])\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "THIS IS WHAT HAPPENS AT EVERY TIME STEP OF THE GRU!\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 120])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([69, 70, 15, 66, 70], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([True, True, True, True, True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(4.7773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  5\n",
            "[23.886330127716064]\n",
            "5\n",
            "Returned Loss =  4.777266025543213\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 120])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([80, 12, 92, 71, 71], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([True, True, True, True, True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(4.7851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  5\n",
            "[23.886330127716064, 23.92545223236084]\n",
            "5\n",
            "Returned Loss =  4.78117823600769\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 120])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([  7, 109, 118, 118, 118], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([True, True, True, True, True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(4.7969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  5\n",
            "[23.886330127716064, 23.92545223236084, 23.984618186950684]\n",
            "5\n",
            "Returned Loss =  4.786426703135173\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 120])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([2, 2, 2, 2, 2], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([True, True, True, True, True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(4.8040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  5\n",
            "[23.886330127716064, 23.92545223236084, 23.984618186950684, 24.019877910614014]\n",
            "5\n",
            "Returned Loss =  4.79081392288208\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLEhn9bh07Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder,\n",
        "          embedding, encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    encoder_output, encoder_hidden = encoder(input_variable, lengths)\n",
        "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses)/n_totals\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIAr5Arr1as3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(model_name, vocab, qa_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, \n",
        "               print_every, save_every, clip, corpus_name, loadFilename):\n",
        "    \n",
        "    training_batches = [batch2TrainData(vocab, [random.choice(qa_pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop FINALLY\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration-1]\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': vocab.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_kVcxnx2VKm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing the trained net\n",
        "# For reading in user input and responding\n",
        "\n",
        "class GreedySearchDecoder(nn.Module): \n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length = 5):\n",
        "\n",
        "        # Encode the input sequence through the encoder model\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "\n",
        "        # Encoder's last hidden state is decoder's first hidden state\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "        # Decoder input starts with SOS_TOKEN\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_TOKEN\n",
        "\n",
        "        # Initialize tensors where the words will be appended after they're found\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "\n",
        "        # Decode one word at a time\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "            \n",
        "            # Get most likely word\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "\n",
        "            # Store the word and score in the tensors\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            \n",
        "            # Prepare current word to be input for the next one\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        return all_tokens, all_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjMwgVa02sb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make input sentence fit for answering, gives it to searcher, gets back the answer, and makes it fit for reading\n",
        "def evaluate(encoder, decoder, searcher, vocab, sentence, max_length = 5):\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence)]\n",
        "\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0,1)\n",
        "\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "\n",
        "    decoded_words = [vocab.index2word[token.item()] for token in tokens]\n",
        "\n",
        "    return decoded_words   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "060nnW4_24dH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing\n",
        "def test(encoder, decoder, searcher, vocab, input_sentence):\n",
        "    try:             \n",
        "        output_words = evaluate(encoder, decoder, searcher, vocab, input_sentence)\n",
        "\n",
        "        output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "        return ' '.join(output_words)\n",
        "\n",
        "    except KeyError:\n",
        "        print(\"Error: Huh. Haven't seen that before.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC0cRmr53N2r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "52c25164-52f3-4c83-ebda-880e73168913"
      },
      "source": [
        "model_name = 'Summer'\n",
        "corpus_name = 'Cornell'\n",
        "attention_model = 'dot'\n",
        "hidden_size = 400\n",
        "encoder_n_layers = 3\n",
        "decoder_n_layers = 3\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "save_dir = os.getcwd()\n",
        "loadFilename = None\n",
        "checkpoint_iter = 5000\n",
        "\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                          '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                          '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    vocab.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "# FINAL FINALLY\n",
        "print('Building the Encoder and Decoder...')\n",
        "\n",
        "embedding = nn.Embedding(vocab.numwords, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "encoder = Encoder(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = Decoder(attention_model, embedding, hidden_size, vocab.numwords, decoder_n_layers, dropout)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "print(\"We're ready to go!\")\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the Encoder and Decoder...\n",
            "We're ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avSjLFqu3c4u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bd27761-303a-4328-97a6-4e2c87bb6c6b"
      },
      "source": [
        "# Training, FINAL FINAL FINALLYYYYY\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 2000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "print(\"Starting Training!\")\n",
        "\n",
        "trainIters(model_name, vocab, qa_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name,loadFilename)\n",
        "\n",
        "print(\"Trained.\")\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing...\n",
            "Training...\n",
            "Iteration: 1; Percent complete: 0.1%; Average loss: 4.7791\n",
            "Iteration: 2; Percent complete: 0.1%; Average loss: 4.6582\n",
            "Iteration: 3; Percent complete: 0.1%; Average loss: 4.5177\n",
            "Iteration: 4; Percent complete: 0.2%; Average loss: 4.3055\n",
            "Iteration: 5; Percent complete: 0.2%; Average loss: 4.0346\n",
            "Iteration: 6; Percent complete: 0.3%; Average loss: 3.7363\n",
            "Iteration: 7; Percent complete: 0.4%; Average loss: 3.6154\n",
            "Iteration: 8; Percent complete: 0.4%; Average loss: 3.6971\n",
            "Iteration: 9; Percent complete: 0.4%; Average loss: 3.5167\n",
            "Iteration: 10; Percent complete: 0.5%; Average loss: 3.3091\n",
            "Iteration: 11; Percent complete: 0.5%; Average loss: 3.2981\n",
            "Iteration: 12; Percent complete: 0.6%; Average loss: 3.2704\n",
            "Iteration: 13; Percent complete: 0.7%; Average loss: 3.3932\n",
            "Iteration: 14; Percent complete: 0.7%; Average loss: 3.2266\n",
            "Iteration: 15; Percent complete: 0.8%; Average loss: 3.2286\n",
            "Iteration: 16; Percent complete: 0.8%; Average loss: 3.1189\n",
            "Iteration: 17; Percent complete: 0.9%; Average loss: 3.0571\n",
            "Iteration: 18; Percent complete: 0.9%; Average loss: 2.9619\n",
            "Iteration: 19; Percent complete: 0.9%; Average loss: 2.9367\n",
            "Iteration: 20; Percent complete: 1.0%; Average loss: 2.7994\n",
            "Iteration: 21; Percent complete: 1.1%; Average loss: 2.7145\n",
            "Iteration: 22; Percent complete: 1.1%; Average loss: 2.6582\n",
            "Iteration: 23; Percent complete: 1.1%; Average loss: 2.6658\n",
            "Iteration: 24; Percent complete: 1.2%; Average loss: 2.5532\n",
            "Iteration: 25; Percent complete: 1.2%; Average loss: 2.6602\n",
            "Iteration: 26; Percent complete: 1.3%; Average loss: 2.4049\n",
            "Iteration: 27; Percent complete: 1.4%; Average loss: 2.4534\n",
            "Iteration: 28; Percent complete: 1.4%; Average loss: 2.3643\n",
            "Iteration: 29; Percent complete: 1.5%; Average loss: 2.4252\n",
            "Iteration: 30; Percent complete: 1.5%; Average loss: 2.3096\n",
            "Iteration: 31; Percent complete: 1.6%; Average loss: 2.2770\n",
            "Iteration: 32; Percent complete: 1.6%; Average loss: 2.1738\n",
            "Iteration: 33; Percent complete: 1.7%; Average loss: 2.2407\n",
            "Iteration: 34; Percent complete: 1.7%; Average loss: 2.2408\n",
            "Iteration: 35; Percent complete: 1.8%; Average loss: 2.1502\n",
            "Iteration: 36; Percent complete: 1.8%; Average loss: 2.2026\n",
            "Iteration: 37; Percent complete: 1.8%; Average loss: 2.2512\n",
            "Iteration: 38; Percent complete: 1.9%; Average loss: 2.1336\n",
            "Iteration: 39; Percent complete: 1.9%; Average loss: 2.1587\n",
            "Iteration: 40; Percent complete: 2.0%; Average loss: 2.1352\n",
            "Iteration: 41; Percent complete: 2.1%; Average loss: 2.0690\n",
            "Iteration: 42; Percent complete: 2.1%; Average loss: 1.9967\n",
            "Iteration: 43; Percent complete: 2.1%; Average loss: 1.9696\n",
            "Iteration: 44; Percent complete: 2.2%; Average loss: 2.1075\n",
            "Iteration: 45; Percent complete: 2.2%; Average loss: 1.9564\n",
            "Iteration: 46; Percent complete: 2.3%; Average loss: 2.1215\n",
            "Iteration: 47; Percent complete: 2.4%; Average loss: 2.0937\n",
            "Iteration: 48; Percent complete: 2.4%; Average loss: 2.0594\n",
            "Iteration: 49; Percent complete: 2.5%; Average loss: 1.9951\n",
            "Iteration: 50; Percent complete: 2.5%; Average loss: 1.9864\n",
            "Iteration: 51; Percent complete: 2.5%; Average loss: 2.0770\n",
            "Iteration: 52; Percent complete: 2.6%; Average loss: 2.0181\n",
            "Iteration: 53; Percent complete: 2.6%; Average loss: 2.0315\n",
            "Iteration: 54; Percent complete: 2.7%; Average loss: 1.9142\n",
            "Iteration: 55; Percent complete: 2.8%; Average loss: 1.8636\n",
            "Iteration: 56; Percent complete: 2.8%; Average loss: 1.9366\n",
            "Iteration: 57; Percent complete: 2.9%; Average loss: 2.1274\n",
            "Iteration: 58; Percent complete: 2.9%; Average loss: 2.0048\n",
            "Iteration: 59; Percent complete: 2.9%; Average loss: 2.0985\n",
            "Iteration: 60; Percent complete: 3.0%; Average loss: 1.8784\n",
            "Iteration: 61; Percent complete: 3.0%; Average loss: 2.0040\n",
            "Iteration: 62; Percent complete: 3.1%; Average loss: 1.9960\n",
            "Iteration: 63; Percent complete: 3.1%; Average loss: 1.9190\n",
            "Iteration: 64; Percent complete: 3.2%; Average loss: 1.8531\n",
            "Iteration: 65; Percent complete: 3.2%; Average loss: 1.9689\n",
            "Iteration: 66; Percent complete: 3.3%; Average loss: 1.9307\n",
            "Iteration: 67; Percent complete: 3.4%; Average loss: 1.9853\n",
            "Iteration: 68; Percent complete: 3.4%; Average loss: 1.9146\n",
            "Iteration: 69; Percent complete: 3.5%; Average loss: 1.9966\n",
            "Iteration: 70; Percent complete: 3.5%; Average loss: 1.9844\n",
            "Iteration: 71; Percent complete: 3.5%; Average loss: 1.9546\n",
            "Iteration: 72; Percent complete: 3.6%; Average loss: 1.9053\n",
            "Iteration: 73; Percent complete: 3.6%; Average loss: 1.9422\n",
            "Iteration: 74; Percent complete: 3.7%; Average loss: 1.9224\n",
            "Iteration: 75; Percent complete: 3.8%; Average loss: 1.8189\n",
            "Iteration: 76; Percent complete: 3.8%; Average loss: 1.7899\n",
            "Iteration: 77; Percent complete: 3.9%; Average loss: 1.8948\n",
            "Iteration: 78; Percent complete: 3.9%; Average loss: 1.9172\n",
            "Iteration: 79; Percent complete: 4.0%; Average loss: 1.9092\n",
            "Iteration: 80; Percent complete: 4.0%; Average loss: 1.9146\n",
            "Iteration: 81; Percent complete: 4.0%; Average loss: 1.8797\n",
            "Iteration: 82; Percent complete: 4.1%; Average loss: 1.9482\n",
            "Iteration: 83; Percent complete: 4.2%; Average loss: 1.8882\n",
            "Iteration: 84; Percent complete: 4.2%; Average loss: 1.8433\n",
            "Iteration: 85; Percent complete: 4.2%; Average loss: 1.8112\n",
            "Iteration: 86; Percent complete: 4.3%; Average loss: 1.7879\n",
            "Iteration: 87; Percent complete: 4.3%; Average loss: 1.8464\n",
            "Iteration: 88; Percent complete: 4.4%; Average loss: 1.9106\n",
            "Iteration: 89; Percent complete: 4.5%; Average loss: 1.7856\n",
            "Iteration: 90; Percent complete: 4.5%; Average loss: 1.9117\n",
            "Iteration: 91; Percent complete: 4.5%; Average loss: 1.7734\n",
            "Iteration: 92; Percent complete: 4.6%; Average loss: 1.8811\n",
            "Iteration: 93; Percent complete: 4.7%; Average loss: 1.8533\n",
            "Iteration: 94; Percent complete: 4.7%; Average loss: 1.8650\n",
            "Iteration: 95; Percent complete: 4.8%; Average loss: 1.7934\n",
            "Iteration: 96; Percent complete: 4.8%; Average loss: 1.8754\n",
            "Iteration: 97; Percent complete: 4.9%; Average loss: 1.7706\n",
            "Iteration: 98; Percent complete: 4.9%; Average loss: 1.8592\n",
            "Iteration: 99; Percent complete: 5.0%; Average loss: 1.7677\n",
            "Iteration: 100; Percent complete: 5.0%; Average loss: 1.8135\n",
            "Iteration: 101; Percent complete: 5.1%; Average loss: 1.7050\n",
            "Iteration: 102; Percent complete: 5.1%; Average loss: 1.7716\n",
            "Iteration: 103; Percent complete: 5.1%; Average loss: 1.7367\n",
            "Iteration: 104; Percent complete: 5.2%; Average loss: 1.7715\n",
            "Iteration: 105; Percent complete: 5.2%; Average loss: 1.6542\n",
            "Iteration: 106; Percent complete: 5.3%; Average loss: 1.6998\n",
            "Iteration: 107; Percent complete: 5.3%; Average loss: 1.7377\n",
            "Iteration: 108; Percent complete: 5.4%; Average loss: 1.7085\n",
            "Iteration: 109; Percent complete: 5.5%; Average loss: 1.7101\n",
            "Iteration: 110; Percent complete: 5.5%; Average loss: 1.6825\n",
            "Iteration: 111; Percent complete: 5.5%; Average loss: 1.7066\n",
            "Iteration: 112; Percent complete: 5.6%; Average loss: 1.6533\n",
            "Iteration: 113; Percent complete: 5.7%; Average loss: 1.6437\n",
            "Iteration: 114; Percent complete: 5.7%; Average loss: 1.5861\n",
            "Iteration: 115; Percent complete: 5.8%; Average loss: 1.5432\n",
            "Iteration: 116; Percent complete: 5.8%; Average loss: 1.6397\n",
            "Iteration: 117; Percent complete: 5.9%; Average loss: 1.5657\n",
            "Iteration: 118; Percent complete: 5.9%; Average loss: 1.6077\n",
            "Iteration: 119; Percent complete: 5.9%; Average loss: 1.5731\n",
            "Iteration: 120; Percent complete: 6.0%; Average loss: 1.5034\n",
            "Iteration: 121; Percent complete: 6.0%; Average loss: 1.5409\n",
            "Iteration: 122; Percent complete: 6.1%; Average loss: 1.5202\n",
            "Iteration: 123; Percent complete: 6.2%; Average loss: 1.4427\n",
            "Iteration: 124; Percent complete: 6.2%; Average loss: 1.4899\n",
            "Iteration: 125; Percent complete: 6.2%; Average loss: 1.4047\n",
            "Iteration: 126; Percent complete: 6.3%; Average loss: 1.4240\n",
            "Iteration: 127; Percent complete: 6.3%; Average loss: 1.4344\n",
            "Iteration: 128; Percent complete: 6.4%; Average loss: 1.3396\n",
            "Iteration: 129; Percent complete: 6.5%; Average loss: 1.2983\n",
            "Iteration: 130; Percent complete: 6.5%; Average loss: 1.3558\n",
            "Iteration: 131; Percent complete: 6.6%; Average loss: 1.3475\n",
            "Iteration: 132; Percent complete: 6.6%; Average loss: 1.2660\n",
            "Iteration: 133; Percent complete: 6.7%; Average loss: 1.2943\n",
            "Iteration: 134; Percent complete: 6.7%; Average loss: 1.2303\n",
            "Iteration: 135; Percent complete: 6.8%; Average loss: 1.1974\n",
            "Iteration: 136; Percent complete: 6.8%; Average loss: 1.1835\n",
            "Iteration: 137; Percent complete: 6.9%; Average loss: 1.1385\n",
            "Iteration: 138; Percent complete: 6.9%; Average loss: 1.1602\n",
            "Iteration: 139; Percent complete: 7.0%; Average loss: 1.1297\n",
            "Iteration: 140; Percent complete: 7.0%; Average loss: 1.0475\n",
            "Iteration: 141; Percent complete: 7.0%; Average loss: 1.0413\n",
            "Iteration: 142; Percent complete: 7.1%; Average loss: 1.0446\n",
            "Iteration: 143; Percent complete: 7.1%; Average loss: 1.0023\n",
            "Iteration: 144; Percent complete: 7.2%; Average loss: 0.9665\n",
            "Iteration: 145; Percent complete: 7.2%; Average loss: 0.9364\n",
            "Iteration: 146; Percent complete: 7.3%; Average loss: 0.8965\n",
            "Iteration: 147; Percent complete: 7.3%; Average loss: 0.8548\n",
            "Iteration: 148; Percent complete: 7.4%; Average loss: 0.8997\n",
            "Iteration: 149; Percent complete: 7.4%; Average loss: 0.7972\n",
            "Iteration: 150; Percent complete: 7.5%; Average loss: 0.7664\n",
            "Iteration: 151; Percent complete: 7.5%; Average loss: 0.6890\n",
            "Iteration: 152; Percent complete: 7.6%; Average loss: 0.6839\n",
            "Iteration: 153; Percent complete: 7.6%; Average loss: 0.7075\n",
            "Iteration: 154; Percent complete: 7.7%; Average loss: 0.6888\n",
            "Iteration: 155; Percent complete: 7.8%; Average loss: 0.6691\n",
            "Iteration: 156; Percent complete: 7.8%; Average loss: 0.6576\n",
            "Iteration: 157; Percent complete: 7.8%; Average loss: 0.6001\n",
            "Iteration: 158; Percent complete: 7.9%; Average loss: 0.6241\n",
            "Iteration: 159; Percent complete: 8.0%; Average loss: 0.5331\n",
            "Iteration: 160; Percent complete: 8.0%; Average loss: 0.5691\n",
            "Iteration: 161; Percent complete: 8.1%; Average loss: 0.5281\n",
            "Iteration: 162; Percent complete: 8.1%; Average loss: 0.5343\n",
            "Iteration: 163; Percent complete: 8.2%; Average loss: 0.4980\n",
            "Iteration: 164; Percent complete: 8.2%; Average loss: 0.5013\n",
            "Iteration: 165; Percent complete: 8.2%; Average loss: 0.4727\n",
            "Iteration: 166; Percent complete: 8.3%; Average loss: 0.4555\n",
            "Iteration: 167; Percent complete: 8.3%; Average loss: 0.4952\n",
            "Iteration: 168; Percent complete: 8.4%; Average loss: 0.4993\n",
            "Iteration: 169; Percent complete: 8.5%; Average loss: 0.4348\n",
            "Iteration: 170; Percent complete: 8.5%; Average loss: 0.4775\n",
            "Iteration: 171; Percent complete: 8.6%; Average loss: 0.4501\n",
            "Iteration: 172; Percent complete: 8.6%; Average loss: 0.3908\n",
            "Iteration: 173; Percent complete: 8.6%; Average loss: 0.4185\n",
            "Iteration: 174; Percent complete: 8.7%; Average loss: 0.3447\n",
            "Iteration: 175; Percent complete: 8.8%; Average loss: 0.3317\n",
            "Iteration: 176; Percent complete: 8.8%; Average loss: 0.2873\n",
            "Iteration: 177; Percent complete: 8.8%; Average loss: 0.4098\n",
            "Iteration: 178; Percent complete: 8.9%; Average loss: 0.4988\n",
            "Iteration: 179; Percent complete: 8.9%; Average loss: 0.3936\n",
            "Iteration: 180; Percent complete: 9.0%; Average loss: 0.3627\n",
            "Iteration: 181; Percent complete: 9.0%; Average loss: 0.4125\n",
            "Iteration: 182; Percent complete: 9.1%; Average loss: 0.3545\n",
            "Iteration: 183; Percent complete: 9.2%; Average loss: 0.3050\n",
            "Iteration: 184; Percent complete: 9.2%; Average loss: 0.3684\n",
            "Iteration: 185; Percent complete: 9.2%; Average loss: 0.3969\n",
            "Iteration: 186; Percent complete: 9.3%; Average loss: 0.2880\n",
            "Iteration: 187; Percent complete: 9.3%; Average loss: 0.3726\n",
            "Iteration: 188; Percent complete: 9.4%; Average loss: 0.3871\n",
            "Iteration: 189; Percent complete: 9.4%; Average loss: 0.3356\n",
            "Iteration: 190; Percent complete: 9.5%; Average loss: 0.4107\n",
            "Iteration: 191; Percent complete: 9.6%; Average loss: 0.3049\n",
            "Iteration: 192; Percent complete: 9.6%; Average loss: 0.2940\n",
            "Iteration: 193; Percent complete: 9.7%; Average loss: 0.3443\n",
            "Iteration: 194; Percent complete: 9.7%; Average loss: 0.4431\n",
            "Iteration: 195; Percent complete: 9.8%; Average loss: 0.2321\n",
            "Iteration: 196; Percent complete: 9.8%; Average loss: 0.2926\n",
            "Iteration: 197; Percent complete: 9.8%; Average loss: 0.1906\n",
            "Iteration: 198; Percent complete: 9.9%; Average loss: 0.0904\n",
            "Iteration: 199; Percent complete: 10.0%; Average loss: 0.0851\n",
            "Iteration: 200; Percent complete: 10.0%; Average loss: 0.2306\n",
            "Iteration: 201; Percent complete: 10.1%; Average loss: 0.0670\n",
            "Iteration: 202; Percent complete: 10.1%; Average loss: 0.0795\n",
            "Iteration: 203; Percent complete: 10.2%; Average loss: 0.0698\n",
            "Iteration: 204; Percent complete: 10.2%; Average loss: 0.0682\n",
            "Iteration: 205; Percent complete: 10.2%; Average loss: 0.0620\n",
            "Iteration: 206; Percent complete: 10.3%; Average loss: 0.0591\n",
            "Iteration: 207; Percent complete: 10.3%; Average loss: 0.0533\n",
            "Iteration: 208; Percent complete: 10.4%; Average loss: 0.0514\n",
            "Iteration: 209; Percent complete: 10.4%; Average loss: 0.0536\n",
            "Iteration: 210; Percent complete: 10.5%; Average loss: 0.0547\n",
            "Iteration: 211; Percent complete: 10.5%; Average loss: 0.0581\n",
            "Iteration: 212; Percent complete: 10.6%; Average loss: 0.0520\n",
            "Iteration: 213; Percent complete: 10.7%; Average loss: 0.0491\n",
            "Iteration: 214; Percent complete: 10.7%; Average loss: 0.0477\n",
            "Iteration: 215; Percent complete: 10.8%; Average loss: 0.0416\n",
            "Iteration: 216; Percent complete: 10.8%; Average loss: 0.0443\n",
            "Iteration: 217; Percent complete: 10.8%; Average loss: 0.0435\n",
            "Iteration: 218; Percent complete: 10.9%; Average loss: 0.0389\n",
            "Iteration: 219; Percent complete: 10.9%; Average loss: 0.0384\n",
            "Iteration: 220; Percent complete: 11.0%; Average loss: 0.0383\n",
            "Iteration: 221; Percent complete: 11.1%; Average loss: 0.0359\n",
            "Iteration: 222; Percent complete: 11.1%; Average loss: 0.0377\n",
            "Iteration: 223; Percent complete: 11.2%; Average loss: 0.0356\n",
            "Iteration: 224; Percent complete: 11.2%; Average loss: 0.0332\n",
            "Iteration: 225; Percent complete: 11.2%; Average loss: 0.0328\n",
            "Iteration: 226; Percent complete: 11.3%; Average loss: 0.0307\n",
            "Iteration: 227; Percent complete: 11.3%; Average loss: 0.0302\n",
            "Iteration: 228; Percent complete: 11.4%; Average loss: 0.0314\n",
            "Iteration: 229; Percent complete: 11.5%; Average loss: 0.0309\n",
            "Iteration: 230; Percent complete: 11.5%; Average loss: 0.0277\n",
            "Iteration: 231; Percent complete: 11.6%; Average loss: 0.0280\n",
            "Iteration: 232; Percent complete: 11.6%; Average loss: 0.0268\n",
            "Iteration: 233; Percent complete: 11.7%; Average loss: 0.0260\n",
            "Iteration: 234; Percent complete: 11.7%; Average loss: 0.0251\n",
            "Iteration: 235; Percent complete: 11.8%; Average loss: 0.0248\n",
            "Iteration: 236; Percent complete: 11.8%; Average loss: 0.0229\n",
            "Iteration: 237; Percent complete: 11.8%; Average loss: 0.0235\n",
            "Iteration: 238; Percent complete: 11.9%; Average loss: 0.0227\n",
            "Iteration: 239; Percent complete: 11.9%; Average loss: 0.0224\n",
            "Iteration: 240; Percent complete: 12.0%; Average loss: 0.0232\n",
            "Iteration: 241; Percent complete: 12.0%; Average loss: 0.0225\n",
            "Iteration: 242; Percent complete: 12.1%; Average loss: 0.0203\n",
            "Iteration: 243; Percent complete: 12.2%; Average loss: 0.0216\n",
            "Iteration: 244; Percent complete: 12.2%; Average loss: 0.0212\n",
            "Iteration: 245; Percent complete: 12.2%; Average loss: 0.0200\n",
            "Iteration: 246; Percent complete: 12.3%; Average loss: 0.0194\n",
            "Iteration: 247; Percent complete: 12.3%; Average loss: 0.0200\n",
            "Iteration: 248; Percent complete: 12.4%; Average loss: 0.0192\n",
            "Iteration: 249; Percent complete: 12.4%; Average loss: 0.0195\n",
            "Iteration: 250; Percent complete: 12.5%; Average loss: 0.0184\n",
            "Iteration: 251; Percent complete: 12.6%; Average loss: 0.0181\n",
            "Iteration: 252; Percent complete: 12.6%; Average loss: 0.0181\n",
            "Iteration: 253; Percent complete: 12.7%; Average loss: 0.0186\n",
            "Iteration: 254; Percent complete: 12.7%; Average loss: 0.0168\n",
            "Iteration: 255; Percent complete: 12.8%; Average loss: 0.0178\n",
            "Iteration: 256; Percent complete: 12.8%; Average loss: 0.0172\n",
            "Iteration: 257; Percent complete: 12.8%; Average loss: 0.0163\n",
            "Iteration: 258; Percent complete: 12.9%; Average loss: 0.0175\n",
            "Iteration: 259; Percent complete: 13.0%; Average loss: 0.0165\n",
            "Iteration: 260; Percent complete: 13.0%; Average loss: 0.0167\n",
            "Iteration: 261; Percent complete: 13.1%; Average loss: 0.0157\n",
            "Iteration: 262; Percent complete: 13.1%; Average loss: 0.0160\n",
            "Iteration: 263; Percent complete: 13.2%; Average loss: 0.0153\n",
            "Iteration: 264; Percent complete: 13.2%; Average loss: 0.0160\n",
            "Iteration: 265; Percent complete: 13.2%; Average loss: 0.0154\n",
            "Iteration: 266; Percent complete: 13.3%; Average loss: 0.0144\n",
            "Iteration: 267; Percent complete: 13.4%; Average loss: 0.0159\n",
            "Iteration: 268; Percent complete: 13.4%; Average loss: 0.0149\n",
            "Iteration: 269; Percent complete: 13.5%; Average loss: 0.0146\n",
            "Iteration: 270; Percent complete: 13.5%; Average loss: 0.0142\n",
            "Iteration: 271; Percent complete: 13.6%; Average loss: 0.0138\n",
            "Iteration: 272; Percent complete: 13.6%; Average loss: 0.0140\n",
            "Iteration: 273; Percent complete: 13.7%; Average loss: 0.0144\n",
            "Iteration: 274; Percent complete: 13.7%; Average loss: 0.0148\n",
            "Iteration: 275; Percent complete: 13.8%; Average loss: 0.0139\n",
            "Iteration: 276; Percent complete: 13.8%; Average loss: 0.0131\n",
            "Iteration: 277; Percent complete: 13.9%; Average loss: 0.0138\n",
            "Iteration: 278; Percent complete: 13.9%; Average loss: 0.0127\n",
            "Iteration: 279; Percent complete: 14.0%; Average loss: 0.0142\n",
            "Iteration: 280; Percent complete: 14.0%; Average loss: 0.0137\n",
            "Iteration: 281; Percent complete: 14.1%; Average loss: 0.0130\n",
            "Iteration: 282; Percent complete: 14.1%; Average loss: 0.0127\n",
            "Iteration: 283; Percent complete: 14.1%; Average loss: 0.0128\n",
            "Iteration: 284; Percent complete: 14.2%; Average loss: 0.0135\n",
            "Iteration: 285; Percent complete: 14.2%; Average loss: 0.0123\n",
            "Iteration: 286; Percent complete: 14.3%; Average loss: 0.0122\n",
            "Iteration: 287; Percent complete: 14.3%; Average loss: 0.0120\n",
            "Iteration: 288; Percent complete: 14.4%; Average loss: 0.0129\n",
            "Iteration: 289; Percent complete: 14.4%; Average loss: 0.0122\n",
            "Iteration: 290; Percent complete: 14.5%; Average loss: 0.0116\n",
            "Iteration: 291; Percent complete: 14.5%; Average loss: 0.0117\n",
            "Iteration: 292; Percent complete: 14.6%; Average loss: 0.0117\n",
            "Iteration: 293; Percent complete: 14.6%; Average loss: 0.0116\n",
            "Iteration: 294; Percent complete: 14.7%; Average loss: 0.0120\n",
            "Iteration: 295; Percent complete: 14.8%; Average loss: 0.0119\n",
            "Iteration: 296; Percent complete: 14.8%; Average loss: 0.0113\n",
            "Iteration: 297; Percent complete: 14.8%; Average loss: 0.0113\n",
            "Iteration: 298; Percent complete: 14.9%; Average loss: 0.0112\n",
            "Iteration: 299; Percent complete: 14.9%; Average loss: 0.0110\n",
            "Iteration: 300; Percent complete: 15.0%; Average loss: 0.0109\n",
            "Iteration: 301; Percent complete: 15.0%; Average loss: 0.0113\n",
            "Iteration: 302; Percent complete: 15.1%; Average loss: 0.0106\n",
            "Iteration: 303; Percent complete: 15.2%; Average loss: 0.0106\n",
            "Iteration: 304; Percent complete: 15.2%; Average loss: 0.0107\n",
            "Iteration: 305; Percent complete: 15.2%; Average loss: 0.0379\n",
            "Iteration: 306; Percent complete: 15.3%; Average loss: 0.0105\n",
            "Iteration: 307; Percent complete: 15.3%; Average loss: 0.0100\n",
            "Iteration: 308; Percent complete: 15.4%; Average loss: 0.0105\n",
            "Iteration: 309; Percent complete: 15.4%; Average loss: 0.0107\n",
            "Iteration: 310; Percent complete: 15.5%; Average loss: 0.0110\n",
            "Iteration: 311; Percent complete: 15.6%; Average loss: 0.0108\n",
            "Iteration: 312; Percent complete: 15.6%; Average loss: 0.0104\n",
            "Iteration: 313; Percent complete: 15.7%; Average loss: 0.0107\n",
            "Iteration: 314; Percent complete: 15.7%; Average loss: 0.0103\n",
            "Iteration: 315; Percent complete: 15.8%; Average loss: 0.0106\n",
            "Iteration: 316; Percent complete: 15.8%; Average loss: 0.0104\n",
            "Iteration: 317; Percent complete: 15.8%; Average loss: 0.0165\n",
            "Iteration: 318; Percent complete: 15.9%; Average loss: 0.0142\n",
            "Iteration: 319; Percent complete: 16.0%; Average loss: 0.0107\n",
            "Iteration: 320; Percent complete: 16.0%; Average loss: 0.0101\n",
            "Iteration: 321; Percent complete: 16.1%; Average loss: 0.0094\n",
            "Iteration: 322; Percent complete: 16.1%; Average loss: 0.0099\n",
            "Iteration: 323; Percent complete: 16.2%; Average loss: 0.0104\n",
            "Iteration: 324; Percent complete: 16.2%; Average loss: 0.0096\n",
            "Iteration: 325; Percent complete: 16.2%; Average loss: 0.0101\n",
            "Iteration: 326; Percent complete: 16.3%; Average loss: 0.0095\n",
            "Iteration: 327; Percent complete: 16.4%; Average loss: 0.0094\n",
            "Iteration: 328; Percent complete: 16.4%; Average loss: 0.0094\n",
            "Iteration: 329; Percent complete: 16.4%; Average loss: 0.0092\n",
            "Iteration: 330; Percent complete: 16.5%; Average loss: 0.0090\n",
            "Iteration: 331; Percent complete: 16.6%; Average loss: 0.0096\n",
            "Iteration: 332; Percent complete: 16.6%; Average loss: 0.0088\n",
            "Iteration: 333; Percent complete: 16.7%; Average loss: 0.0082\n",
            "Iteration: 334; Percent complete: 16.7%; Average loss: 0.0091\n",
            "Iteration: 335; Percent complete: 16.8%; Average loss: 0.0084\n",
            "Iteration: 336; Percent complete: 16.8%; Average loss: 0.0090\n",
            "Iteration: 337; Percent complete: 16.9%; Average loss: 0.0088\n",
            "Iteration: 338; Percent complete: 16.9%; Average loss: 0.0086\n",
            "Iteration: 339; Percent complete: 17.0%; Average loss: 0.0103\n",
            "Iteration: 340; Percent complete: 17.0%; Average loss: 0.0092\n",
            "Iteration: 341; Percent complete: 17.1%; Average loss: 0.0087\n",
            "Iteration: 342; Percent complete: 17.1%; Average loss: 0.0081\n",
            "Iteration: 343; Percent complete: 17.2%; Average loss: 0.0082\n",
            "Iteration: 344; Percent complete: 17.2%; Average loss: 0.0084\n",
            "Iteration: 345; Percent complete: 17.2%; Average loss: 0.0083\n",
            "Iteration: 346; Percent complete: 17.3%; Average loss: 0.0080\n",
            "Iteration: 347; Percent complete: 17.3%; Average loss: 0.0084\n",
            "Iteration: 348; Percent complete: 17.4%; Average loss: 0.0081\n",
            "Iteration: 349; Percent complete: 17.4%; Average loss: 0.0082\n",
            "Iteration: 350; Percent complete: 17.5%; Average loss: 0.0081\n",
            "Iteration: 351; Percent complete: 17.5%; Average loss: 0.0079\n",
            "Iteration: 352; Percent complete: 17.6%; Average loss: 0.0078\n",
            "Iteration: 353; Percent complete: 17.6%; Average loss: 0.0076\n",
            "Iteration: 354; Percent complete: 17.7%; Average loss: 0.0077\n",
            "Iteration: 355; Percent complete: 17.8%; Average loss: 0.0076\n",
            "Iteration: 356; Percent complete: 17.8%; Average loss: 0.0077\n",
            "Iteration: 357; Percent complete: 17.8%; Average loss: 0.0074\n",
            "Iteration: 358; Percent complete: 17.9%; Average loss: 0.0071\n",
            "Iteration: 359; Percent complete: 17.9%; Average loss: 0.0075\n",
            "Iteration: 360; Percent complete: 18.0%; Average loss: 0.0074\n",
            "Iteration: 361; Percent complete: 18.1%; Average loss: 0.0076\n",
            "Iteration: 362; Percent complete: 18.1%; Average loss: 0.0071\n",
            "Iteration: 363; Percent complete: 18.1%; Average loss: 0.0073\n",
            "Iteration: 364; Percent complete: 18.2%; Average loss: 0.0069\n",
            "Iteration: 365; Percent complete: 18.2%; Average loss: 0.0073\n",
            "Iteration: 366; Percent complete: 18.3%; Average loss: 0.0071\n",
            "Iteration: 367; Percent complete: 18.4%; Average loss: 0.0072\n",
            "Iteration: 368; Percent complete: 18.4%; Average loss: 0.0069\n",
            "Iteration: 369; Percent complete: 18.4%; Average loss: 0.0069\n",
            "Iteration: 370; Percent complete: 18.5%; Average loss: 0.0073\n",
            "Iteration: 371; Percent complete: 18.6%; Average loss: 0.0071\n",
            "Iteration: 372; Percent complete: 18.6%; Average loss: 0.0069\n",
            "Iteration: 373; Percent complete: 18.6%; Average loss: 0.0069\n",
            "Iteration: 374; Percent complete: 18.7%; Average loss: 0.0071\n",
            "Iteration: 375; Percent complete: 18.8%; Average loss: 0.0068\n",
            "Iteration: 376; Percent complete: 18.8%; Average loss: 0.0068\n",
            "Iteration: 377; Percent complete: 18.9%; Average loss: 0.0067\n",
            "Iteration: 378; Percent complete: 18.9%; Average loss: 0.0068\n",
            "Iteration: 379; Percent complete: 18.9%; Average loss: 0.0068\n",
            "Iteration: 380; Percent complete: 19.0%; Average loss: 0.0067\n",
            "Iteration: 381; Percent complete: 19.1%; Average loss: 0.0065\n",
            "Iteration: 382; Percent complete: 19.1%; Average loss: 0.0067\n",
            "Iteration: 383; Percent complete: 19.1%; Average loss: 0.0066\n",
            "Iteration: 384; Percent complete: 19.2%; Average loss: 0.0064\n",
            "Iteration: 385; Percent complete: 19.2%; Average loss: 0.0065\n",
            "Iteration: 386; Percent complete: 19.3%; Average loss: 0.0065\n",
            "Iteration: 387; Percent complete: 19.4%; Average loss: 0.0063\n",
            "Iteration: 388; Percent complete: 19.4%; Average loss: 0.0060\n",
            "Iteration: 389; Percent complete: 19.4%; Average loss: 0.0059\n",
            "Iteration: 390; Percent complete: 19.5%; Average loss: 0.0066\n",
            "Iteration: 391; Percent complete: 19.6%; Average loss: 0.0063\n",
            "Iteration: 392; Percent complete: 19.6%; Average loss: 0.0060\n",
            "Iteration: 393; Percent complete: 19.7%; Average loss: 0.0059\n",
            "Iteration: 394; Percent complete: 19.7%; Average loss: 0.0061\n",
            "Iteration: 395; Percent complete: 19.8%; Average loss: 0.0061\n",
            "Iteration: 396; Percent complete: 19.8%; Average loss: 0.0058\n",
            "Iteration: 397; Percent complete: 19.9%; Average loss: 0.0060\n",
            "Iteration: 398; Percent complete: 19.9%; Average loss: 0.0060\n",
            "Iteration: 399; Percent complete: 20.0%; Average loss: 0.0057\n",
            "Iteration: 400; Percent complete: 20.0%; Average loss: 0.0058\n",
            "Iteration: 401; Percent complete: 20.1%; Average loss: 0.0059\n",
            "Iteration: 402; Percent complete: 20.1%; Average loss: 0.0057\n",
            "Iteration: 403; Percent complete: 20.2%; Average loss: 0.0057\n",
            "Iteration: 404; Percent complete: 20.2%; Average loss: 0.0058\n",
            "Iteration: 405; Percent complete: 20.2%; Average loss: 0.0059\n",
            "Iteration: 406; Percent complete: 20.3%; Average loss: 0.0056\n",
            "Iteration: 407; Percent complete: 20.3%; Average loss: 0.0055\n",
            "Iteration: 408; Percent complete: 20.4%; Average loss: 0.0060\n",
            "Iteration: 409; Percent complete: 20.4%; Average loss: 0.0055\n",
            "Iteration: 410; Percent complete: 20.5%; Average loss: 0.0056\n",
            "Iteration: 411; Percent complete: 20.5%; Average loss: 0.0057\n",
            "Iteration: 412; Percent complete: 20.6%; Average loss: 0.0055\n",
            "Iteration: 413; Percent complete: 20.6%; Average loss: 0.0057\n",
            "Iteration: 414; Percent complete: 20.7%; Average loss: 0.0057\n",
            "Iteration: 415; Percent complete: 20.8%; Average loss: 0.0054\n",
            "Iteration: 416; Percent complete: 20.8%; Average loss: 0.0052\n",
            "Iteration: 417; Percent complete: 20.8%; Average loss: 0.0054\n",
            "Iteration: 418; Percent complete: 20.9%; Average loss: 0.0052\n",
            "Iteration: 419; Percent complete: 20.9%; Average loss: 0.0053\n",
            "Iteration: 420; Percent complete: 21.0%; Average loss: 0.0056\n",
            "Iteration: 421; Percent complete: 21.1%; Average loss: 0.0052\n",
            "Iteration: 422; Percent complete: 21.1%; Average loss: 0.0053\n",
            "Iteration: 423; Percent complete: 21.1%; Average loss: 0.0052\n",
            "Iteration: 424; Percent complete: 21.2%; Average loss: 0.0053\n",
            "Iteration: 425; Percent complete: 21.2%; Average loss: 0.0052\n",
            "Iteration: 426; Percent complete: 21.3%; Average loss: 0.0052\n",
            "Iteration: 427; Percent complete: 21.3%; Average loss: 0.0051\n",
            "Iteration: 428; Percent complete: 21.4%; Average loss: 0.0054\n",
            "Iteration: 429; Percent complete: 21.4%; Average loss: 0.0050\n",
            "Iteration: 430; Percent complete: 21.5%; Average loss: 0.0051\n",
            "Iteration: 431; Percent complete: 21.6%; Average loss: 0.0051\n",
            "Iteration: 432; Percent complete: 21.6%; Average loss: 0.0053\n",
            "Iteration: 433; Percent complete: 21.6%; Average loss: 0.0051\n",
            "Iteration: 434; Percent complete: 21.7%; Average loss: 0.0051\n",
            "Iteration: 435; Percent complete: 21.8%; Average loss: 0.0052\n",
            "Iteration: 436; Percent complete: 21.8%; Average loss: 0.0052\n",
            "Iteration: 437; Percent complete: 21.9%; Average loss: 0.0051\n",
            "Iteration: 438; Percent complete: 21.9%; Average loss: 0.0048\n",
            "Iteration: 439; Percent complete: 21.9%; Average loss: 0.0047\n",
            "Iteration: 440; Percent complete: 22.0%; Average loss: 0.0047\n",
            "Iteration: 441; Percent complete: 22.1%; Average loss: 0.0048\n",
            "Iteration: 442; Percent complete: 22.1%; Average loss: 0.0048\n",
            "Iteration: 443; Percent complete: 22.1%; Average loss: 0.0047\n",
            "Iteration: 444; Percent complete: 22.2%; Average loss: 0.0050\n",
            "Iteration: 445; Percent complete: 22.2%; Average loss: 0.0048\n",
            "Iteration: 446; Percent complete: 22.3%; Average loss: 0.0048\n",
            "Iteration: 447; Percent complete: 22.4%; Average loss: 0.0049\n",
            "Iteration: 448; Percent complete: 22.4%; Average loss: 0.0046\n",
            "Iteration: 449; Percent complete: 22.4%; Average loss: 0.0047\n",
            "Iteration: 450; Percent complete: 22.5%; Average loss: 0.0048\n",
            "Iteration: 451; Percent complete: 22.6%; Average loss: 0.0048\n",
            "Iteration: 452; Percent complete: 22.6%; Average loss: 0.0046\n",
            "Iteration: 453; Percent complete: 22.7%; Average loss: 0.0046\n",
            "Iteration: 454; Percent complete: 22.7%; Average loss: 0.0044\n",
            "Iteration: 455; Percent complete: 22.8%; Average loss: 0.0046\n",
            "Iteration: 456; Percent complete: 22.8%; Average loss: 0.0044\n",
            "Iteration: 457; Percent complete: 22.9%; Average loss: 0.0045\n",
            "Iteration: 458; Percent complete: 22.9%; Average loss: 0.0045\n",
            "Iteration: 459; Percent complete: 22.9%; Average loss: 0.0046\n",
            "Iteration: 460; Percent complete: 23.0%; Average loss: 0.0044\n",
            "Iteration: 461; Percent complete: 23.1%; Average loss: 0.0043\n",
            "Iteration: 462; Percent complete: 23.1%; Average loss: 0.0042\n",
            "Iteration: 463; Percent complete: 23.2%; Average loss: 0.0043\n",
            "Iteration: 464; Percent complete: 23.2%; Average loss: 0.0045\n",
            "Iteration: 465; Percent complete: 23.2%; Average loss: 0.0044\n",
            "Iteration: 466; Percent complete: 23.3%; Average loss: 0.0045\n",
            "Iteration: 467; Percent complete: 23.4%; Average loss: 0.0044\n",
            "Iteration: 468; Percent complete: 23.4%; Average loss: 0.0042\n",
            "Iteration: 469; Percent complete: 23.4%; Average loss: 0.0045\n",
            "Iteration: 470; Percent complete: 23.5%; Average loss: 0.0042\n",
            "Iteration: 471; Percent complete: 23.5%; Average loss: 0.0042\n",
            "Iteration: 472; Percent complete: 23.6%; Average loss: 0.0043\n",
            "Iteration: 473; Percent complete: 23.6%; Average loss: 0.0043\n",
            "Iteration: 474; Percent complete: 23.7%; Average loss: 0.0041\n",
            "Iteration: 475; Percent complete: 23.8%; Average loss: 0.0041\n",
            "Iteration: 476; Percent complete: 23.8%; Average loss: 0.0042\n",
            "Iteration: 477; Percent complete: 23.8%; Average loss: 0.0040\n",
            "Iteration: 478; Percent complete: 23.9%; Average loss: 0.0043\n",
            "Iteration: 479; Percent complete: 23.9%; Average loss: 0.0040\n",
            "Iteration: 480; Percent complete: 24.0%; Average loss: 0.0041\n",
            "Iteration: 481; Percent complete: 24.1%; Average loss: 0.0041\n",
            "Iteration: 482; Percent complete: 24.1%; Average loss: 0.0040\n",
            "Iteration: 483; Percent complete: 24.1%; Average loss: 0.0041\n",
            "Iteration: 484; Percent complete: 24.2%; Average loss: 0.0041\n",
            "Iteration: 485; Percent complete: 24.2%; Average loss: 0.0039\n",
            "Iteration: 486; Percent complete: 24.3%; Average loss: 0.0040\n",
            "Iteration: 487; Percent complete: 24.3%; Average loss: 0.0039\n",
            "Iteration: 488; Percent complete: 24.4%; Average loss: 0.0038\n",
            "Iteration: 489; Percent complete: 24.4%; Average loss: 0.0040\n",
            "Iteration: 490; Percent complete: 24.5%; Average loss: 0.0041\n",
            "Iteration: 491; Percent complete: 24.6%; Average loss: 0.0039\n",
            "Iteration: 492; Percent complete: 24.6%; Average loss: 0.0038\n",
            "Iteration: 493; Percent complete: 24.6%; Average loss: 0.0041\n",
            "Iteration: 494; Percent complete: 24.7%; Average loss: 0.0038\n",
            "Iteration: 495; Percent complete: 24.8%; Average loss: 0.0039\n",
            "Iteration: 496; Percent complete: 24.8%; Average loss: 0.0039\n",
            "Iteration: 497; Percent complete: 24.9%; Average loss: 0.0037\n",
            "Iteration: 498; Percent complete: 24.9%; Average loss: 0.0038\n",
            "Iteration: 499; Percent complete: 24.9%; Average loss: 0.0037\n",
            "Iteration: 500; Percent complete: 25.0%; Average loss: 0.0038\n",
            "Iteration: 501; Percent complete: 25.1%; Average loss: 0.0037\n",
            "Iteration: 502; Percent complete: 25.1%; Average loss: 0.0037\n",
            "Iteration: 503; Percent complete: 25.1%; Average loss: 0.0037\n",
            "Iteration: 504; Percent complete: 25.2%; Average loss: 0.0037\n",
            "Iteration: 505; Percent complete: 25.2%; Average loss: 0.0036\n",
            "Iteration: 506; Percent complete: 25.3%; Average loss: 0.0038\n",
            "Iteration: 507; Percent complete: 25.4%; Average loss: 0.0039\n",
            "Iteration: 508; Percent complete: 25.4%; Average loss: 0.0036\n",
            "Iteration: 509; Percent complete: 25.4%; Average loss: 0.0035\n",
            "Iteration: 510; Percent complete: 25.5%; Average loss: 0.0037\n",
            "Iteration: 511; Percent complete: 25.6%; Average loss: 0.0036\n",
            "Iteration: 512; Percent complete: 25.6%; Average loss: 0.0036\n",
            "Iteration: 513; Percent complete: 25.7%; Average loss: 0.0037\n",
            "Iteration: 514; Percent complete: 25.7%; Average loss: 0.0035\n",
            "Iteration: 515; Percent complete: 25.8%; Average loss: 0.0037\n",
            "Iteration: 516; Percent complete: 25.8%; Average loss: 0.0035\n",
            "Iteration: 517; Percent complete: 25.9%; Average loss: 0.0035\n",
            "Iteration: 518; Percent complete: 25.9%; Average loss: 0.0036\n",
            "Iteration: 519; Percent complete: 25.9%; Average loss: 0.0035\n",
            "Iteration: 520; Percent complete: 26.0%; Average loss: 0.0037\n",
            "Iteration: 521; Percent complete: 26.1%; Average loss: 0.0035\n",
            "Iteration: 522; Percent complete: 26.1%; Average loss: 0.0033\n",
            "Iteration: 523; Percent complete: 26.2%; Average loss: 0.0035\n",
            "Iteration: 524; Percent complete: 26.2%; Average loss: 0.0036\n",
            "Iteration: 525; Percent complete: 26.2%; Average loss: 0.0036\n",
            "Iteration: 526; Percent complete: 26.3%; Average loss: 0.0035\n",
            "Iteration: 527; Percent complete: 26.4%; Average loss: 0.0034\n",
            "Iteration: 528; Percent complete: 26.4%; Average loss: 0.0035\n",
            "Iteration: 529; Percent complete: 26.5%; Average loss: 0.0035\n",
            "Iteration: 530; Percent complete: 26.5%; Average loss: 0.0034\n",
            "Iteration: 531; Percent complete: 26.6%; Average loss: 0.0033\n",
            "Iteration: 532; Percent complete: 26.6%; Average loss: 0.0033\n",
            "Iteration: 533; Percent complete: 26.7%; Average loss: 0.0034\n",
            "Iteration: 534; Percent complete: 26.7%; Average loss: 0.0034\n",
            "Iteration: 535; Percent complete: 26.8%; Average loss: 0.0034\n",
            "Iteration: 536; Percent complete: 26.8%; Average loss: 0.0033\n",
            "Iteration: 537; Percent complete: 26.9%; Average loss: 0.0034\n",
            "Iteration: 538; Percent complete: 26.9%; Average loss: 0.0033\n",
            "Iteration: 539; Percent complete: 27.0%; Average loss: 0.0033\n",
            "Iteration: 540; Percent complete: 27.0%; Average loss: 0.0033\n",
            "Iteration: 541; Percent complete: 27.1%; Average loss: 0.0033\n",
            "Iteration: 542; Percent complete: 27.1%; Average loss: 0.0032\n",
            "Iteration: 543; Percent complete: 27.2%; Average loss: 0.0032\n",
            "Iteration: 544; Percent complete: 27.2%; Average loss: 0.0033\n",
            "Iteration: 545; Percent complete: 27.3%; Average loss: 0.0033\n",
            "Iteration: 546; Percent complete: 27.3%; Average loss: 0.0031\n",
            "Iteration: 547; Percent complete: 27.4%; Average loss: 0.0032\n",
            "Iteration: 548; Percent complete: 27.4%; Average loss: 0.0031\n",
            "Iteration: 549; Percent complete: 27.5%; Average loss: 0.0032\n",
            "Iteration: 550; Percent complete: 27.5%; Average loss: 0.0030\n",
            "Iteration: 551; Percent complete: 27.6%; Average loss: 0.0031\n",
            "Iteration: 552; Percent complete: 27.6%; Average loss: 0.0031\n",
            "Iteration: 553; Percent complete: 27.7%; Average loss: 0.0032\n",
            "Iteration: 554; Percent complete: 27.7%; Average loss: 0.0033\n",
            "Iteration: 555; Percent complete: 27.8%; Average loss: 0.0031\n",
            "Iteration: 556; Percent complete: 27.8%; Average loss: 0.0032\n",
            "Iteration: 557; Percent complete: 27.9%; Average loss: 0.0031\n",
            "Iteration: 558; Percent complete: 27.9%; Average loss: 0.0030\n",
            "Iteration: 559; Percent complete: 28.0%; Average loss: 0.0030\n",
            "Iteration: 560; Percent complete: 28.0%; Average loss: 0.0032\n",
            "Iteration: 561; Percent complete: 28.1%; Average loss: 0.0030\n",
            "Iteration: 562; Percent complete: 28.1%; Average loss: 0.0030\n",
            "Iteration: 563; Percent complete: 28.1%; Average loss: 0.0031\n",
            "Iteration: 564; Percent complete: 28.2%; Average loss: 0.0030\n",
            "Iteration: 565; Percent complete: 28.2%; Average loss: 0.0029\n",
            "Iteration: 566; Percent complete: 28.3%; Average loss: 0.0029\n",
            "Iteration: 567; Percent complete: 28.3%; Average loss: 0.0030\n",
            "Iteration: 568; Percent complete: 28.4%; Average loss: 0.0031\n",
            "Iteration: 569; Percent complete: 28.4%; Average loss: 0.0029\n",
            "Iteration: 570; Percent complete: 28.5%; Average loss: 0.0030\n",
            "Iteration: 571; Percent complete: 28.5%; Average loss: 0.0030\n",
            "Iteration: 572; Percent complete: 28.6%; Average loss: 0.0031\n",
            "Iteration: 573; Percent complete: 28.6%; Average loss: 0.0029\n",
            "Iteration: 574; Percent complete: 28.7%; Average loss: 0.0029\n",
            "Iteration: 575; Percent complete: 28.7%; Average loss: 0.0029\n",
            "Iteration: 576; Percent complete: 28.8%; Average loss: 0.0029\n",
            "Iteration: 577; Percent complete: 28.8%; Average loss: 0.0029\n",
            "Iteration: 578; Percent complete: 28.9%; Average loss: 0.0028\n",
            "Iteration: 579; Percent complete: 28.9%; Average loss: 0.0029\n",
            "Iteration: 580; Percent complete: 29.0%; Average loss: 0.0029\n",
            "Iteration: 581; Percent complete: 29.0%; Average loss: 0.0028\n",
            "Iteration: 582; Percent complete: 29.1%; Average loss: 0.0029\n",
            "Iteration: 583; Percent complete: 29.1%; Average loss: 0.0028\n",
            "Iteration: 584; Percent complete: 29.2%; Average loss: 0.0028\n",
            "Iteration: 585; Percent complete: 29.2%; Average loss: 0.0027\n",
            "Iteration: 586; Percent complete: 29.3%; Average loss: 0.0027\n",
            "Iteration: 587; Percent complete: 29.3%; Average loss: 0.0028\n",
            "Iteration: 588; Percent complete: 29.4%; Average loss: 0.0026\n",
            "Iteration: 589; Percent complete: 29.4%; Average loss: 0.0028\n",
            "Iteration: 590; Percent complete: 29.5%; Average loss: 0.0028\n",
            "Iteration: 591; Percent complete: 29.5%; Average loss: 0.0028\n",
            "Iteration: 592; Percent complete: 29.6%; Average loss: 0.0028\n",
            "Iteration: 593; Percent complete: 29.6%; Average loss: 0.0027\n",
            "Iteration: 594; Percent complete: 29.7%; Average loss: 0.0026\n",
            "Iteration: 595; Percent complete: 29.8%; Average loss: 0.0029\n",
            "Iteration: 596; Percent complete: 29.8%; Average loss: 0.0029\n",
            "Iteration: 597; Percent complete: 29.8%; Average loss: 0.0027\n",
            "Iteration: 598; Percent complete: 29.9%; Average loss: 0.0028\n",
            "Iteration: 599; Percent complete: 29.9%; Average loss: 0.0028\n",
            "Iteration: 600; Percent complete: 30.0%; Average loss: 0.0026\n",
            "Iteration: 601; Percent complete: 30.0%; Average loss: 0.0026\n",
            "Iteration: 602; Percent complete: 30.1%; Average loss: 0.0026\n",
            "Iteration: 603; Percent complete: 30.1%; Average loss: 0.0027\n",
            "Iteration: 604; Percent complete: 30.2%; Average loss: 0.0027\n",
            "Iteration: 605; Percent complete: 30.2%; Average loss: 0.0027\n",
            "Iteration: 606; Percent complete: 30.3%; Average loss: 0.0025\n",
            "Iteration: 607; Percent complete: 30.3%; Average loss: 0.0027\n",
            "Iteration: 608; Percent complete: 30.4%; Average loss: 0.0026\n",
            "Iteration: 609; Percent complete: 30.4%; Average loss: 0.0026\n",
            "Iteration: 610; Percent complete: 30.5%; Average loss: 0.0026\n",
            "Iteration: 611; Percent complete: 30.6%; Average loss: 0.0026\n",
            "Iteration: 612; Percent complete: 30.6%; Average loss: 0.0026\n",
            "Iteration: 613; Percent complete: 30.6%; Average loss: 0.0025\n",
            "Iteration: 614; Percent complete: 30.7%; Average loss: 0.0026\n",
            "Iteration: 615; Percent complete: 30.8%; Average loss: 0.0025\n",
            "Iteration: 616; Percent complete: 30.8%; Average loss: 0.0025\n",
            "Iteration: 617; Percent complete: 30.9%; Average loss: 0.0026\n",
            "Iteration: 618; Percent complete: 30.9%; Average loss: 0.0025\n",
            "Iteration: 619; Percent complete: 30.9%; Average loss: 0.0025\n",
            "Iteration: 620; Percent complete: 31.0%; Average loss: 0.0025\n",
            "Iteration: 621; Percent complete: 31.1%; Average loss: 0.0025\n",
            "Iteration: 622; Percent complete: 31.1%; Average loss: 0.0025\n",
            "Iteration: 623; Percent complete: 31.1%; Average loss: 0.0026\n",
            "Iteration: 624; Percent complete: 31.2%; Average loss: 0.0025\n",
            "Iteration: 625; Percent complete: 31.2%; Average loss: 0.0025\n",
            "Iteration: 626; Percent complete: 31.3%; Average loss: 0.0024\n",
            "Iteration: 627; Percent complete: 31.4%; Average loss: 0.0025\n",
            "Iteration: 628; Percent complete: 31.4%; Average loss: 0.0025\n",
            "Iteration: 629; Percent complete: 31.4%; Average loss: 0.0026\n",
            "Iteration: 630; Percent complete: 31.5%; Average loss: 0.0025\n",
            "Iteration: 631; Percent complete: 31.6%; Average loss: 0.0024\n",
            "Iteration: 632; Percent complete: 31.6%; Average loss: 0.0024\n",
            "Iteration: 633; Percent complete: 31.6%; Average loss: 0.0024\n",
            "Iteration: 634; Percent complete: 31.7%; Average loss: 0.0025\n",
            "Iteration: 635; Percent complete: 31.8%; Average loss: 0.0024\n",
            "Iteration: 636; Percent complete: 31.8%; Average loss: 0.0023\n",
            "Iteration: 637; Percent complete: 31.9%; Average loss: 0.0025\n",
            "Iteration: 638; Percent complete: 31.9%; Average loss: 0.0025\n",
            "Iteration: 639; Percent complete: 31.9%; Average loss: 0.0024\n",
            "Iteration: 640; Percent complete: 32.0%; Average loss: 0.0025\n",
            "Iteration: 641; Percent complete: 32.0%; Average loss: 0.0023\n",
            "Iteration: 642; Percent complete: 32.1%; Average loss: 0.0025\n",
            "Iteration: 643; Percent complete: 32.1%; Average loss: 0.0026\n",
            "Iteration: 644; Percent complete: 32.2%; Average loss: 0.0024\n",
            "Iteration: 645; Percent complete: 32.2%; Average loss: 0.0024\n",
            "Iteration: 646; Percent complete: 32.3%; Average loss: 0.0022\n",
            "Iteration: 647; Percent complete: 32.4%; Average loss: 0.0024\n",
            "Iteration: 648; Percent complete: 32.4%; Average loss: 0.0023\n",
            "Iteration: 649; Percent complete: 32.5%; Average loss: 0.0024\n",
            "Iteration: 650; Percent complete: 32.5%; Average loss: 0.0023\n",
            "Iteration: 651; Percent complete: 32.6%; Average loss: 0.0023\n",
            "Iteration: 652; Percent complete: 32.6%; Average loss: 0.0022\n",
            "Iteration: 653; Percent complete: 32.6%; Average loss: 0.0023\n",
            "Iteration: 654; Percent complete: 32.7%; Average loss: 0.0022\n",
            "Iteration: 655; Percent complete: 32.8%; Average loss: 0.0023\n",
            "Iteration: 656; Percent complete: 32.8%; Average loss: 0.0023\n",
            "Iteration: 657; Percent complete: 32.9%; Average loss: 0.0023\n",
            "Iteration: 658; Percent complete: 32.9%; Average loss: 0.0022\n",
            "Iteration: 659; Percent complete: 33.0%; Average loss: 0.0022\n",
            "Iteration: 660; Percent complete: 33.0%; Average loss: 0.0023\n",
            "Iteration: 661; Percent complete: 33.1%; Average loss: 0.0022\n",
            "Iteration: 662; Percent complete: 33.1%; Average loss: 0.0022\n",
            "Iteration: 663; Percent complete: 33.1%; Average loss: 0.0023\n",
            "Iteration: 664; Percent complete: 33.2%; Average loss: 0.0022\n",
            "Iteration: 665; Percent complete: 33.2%; Average loss: 0.0022\n",
            "Iteration: 666; Percent complete: 33.3%; Average loss: 0.0022\n",
            "Iteration: 667; Percent complete: 33.4%; Average loss: 0.0023\n",
            "Iteration: 668; Percent complete: 33.4%; Average loss: 0.0022\n",
            "Iteration: 669; Percent complete: 33.5%; Average loss: 0.0022\n",
            "Iteration: 670; Percent complete: 33.5%; Average loss: 0.0022\n",
            "Iteration: 671; Percent complete: 33.6%; Average loss: 0.0023\n",
            "Iteration: 672; Percent complete: 33.6%; Average loss: 0.0023\n",
            "Iteration: 673; Percent complete: 33.7%; Average loss: 0.0020\n",
            "Iteration: 674; Percent complete: 33.7%; Average loss: 0.0021\n",
            "Iteration: 675; Percent complete: 33.8%; Average loss: 0.0022\n",
            "Iteration: 676; Percent complete: 33.8%; Average loss: 0.0022\n",
            "Iteration: 677; Percent complete: 33.9%; Average loss: 0.0021\n",
            "Iteration: 678; Percent complete: 33.9%; Average loss: 0.0021\n",
            "Iteration: 679; Percent complete: 34.0%; Average loss: 0.0020\n",
            "Iteration: 680; Percent complete: 34.0%; Average loss: 0.0022\n",
            "Iteration: 681; Percent complete: 34.1%; Average loss: 0.0022\n",
            "Iteration: 682; Percent complete: 34.1%; Average loss: 0.0021\n",
            "Iteration: 683; Percent complete: 34.2%; Average loss: 0.0022\n",
            "Iteration: 684; Percent complete: 34.2%; Average loss: 0.0021\n",
            "Iteration: 685; Percent complete: 34.2%; Average loss: 0.0021\n",
            "Iteration: 686; Percent complete: 34.3%; Average loss: 0.0020\n",
            "Iteration: 687; Percent complete: 34.4%; Average loss: 0.0021\n",
            "Iteration: 688; Percent complete: 34.4%; Average loss: 0.0021\n",
            "Iteration: 689; Percent complete: 34.4%; Average loss: 0.0021\n",
            "Iteration: 690; Percent complete: 34.5%; Average loss: 0.0020\n",
            "Iteration: 691; Percent complete: 34.5%; Average loss: 0.0021\n",
            "Iteration: 692; Percent complete: 34.6%; Average loss: 0.0021\n",
            "Iteration: 693; Percent complete: 34.6%; Average loss: 0.0021\n",
            "Iteration: 694; Percent complete: 34.7%; Average loss: 0.0021\n",
            "Iteration: 695; Percent complete: 34.8%; Average loss: 0.0021\n",
            "Iteration: 696; Percent complete: 34.8%; Average loss: 0.0021\n",
            "Iteration: 697; Percent complete: 34.8%; Average loss: 0.0021\n",
            "Iteration: 698; Percent complete: 34.9%; Average loss: 0.0021\n",
            "Iteration: 699; Percent complete: 34.9%; Average loss: 0.0021\n",
            "Iteration: 700; Percent complete: 35.0%; Average loss: 0.0021\n",
            "Iteration: 701; Percent complete: 35.0%; Average loss: 0.0020\n",
            "Iteration: 702; Percent complete: 35.1%; Average loss: 0.0021\n",
            "Iteration: 703; Percent complete: 35.1%; Average loss: 0.0020\n",
            "Iteration: 704; Percent complete: 35.2%; Average loss: 0.0021\n",
            "Iteration: 705; Percent complete: 35.2%; Average loss: 0.0020\n",
            "Iteration: 706; Percent complete: 35.3%; Average loss: 0.0020\n",
            "Iteration: 707; Percent complete: 35.4%; Average loss: 0.0021\n",
            "Iteration: 708; Percent complete: 35.4%; Average loss: 0.0020\n",
            "Iteration: 709; Percent complete: 35.4%; Average loss: 0.0019\n",
            "Iteration: 710; Percent complete: 35.5%; Average loss: 0.0021\n",
            "Iteration: 711; Percent complete: 35.5%; Average loss: 0.0020\n",
            "Iteration: 712; Percent complete: 35.6%; Average loss: 0.0020\n",
            "Iteration: 713; Percent complete: 35.6%; Average loss: 0.0020\n",
            "Iteration: 714; Percent complete: 35.7%; Average loss: 0.0020\n",
            "Iteration: 715; Percent complete: 35.8%; Average loss: 0.0020\n",
            "Iteration: 716; Percent complete: 35.8%; Average loss: 0.0020\n",
            "Iteration: 717; Percent complete: 35.9%; Average loss: 0.0019\n",
            "Iteration: 718; Percent complete: 35.9%; Average loss: 0.0019\n",
            "Iteration: 719; Percent complete: 35.9%; Average loss: 0.0020\n",
            "Iteration: 720; Percent complete: 36.0%; Average loss: 0.0019\n",
            "Iteration: 721; Percent complete: 36.0%; Average loss: 0.0019\n",
            "Iteration: 722; Percent complete: 36.1%; Average loss: 0.0020\n",
            "Iteration: 723; Percent complete: 36.1%; Average loss: 0.0020\n",
            "Iteration: 724; Percent complete: 36.2%; Average loss: 0.0021\n",
            "Iteration: 725; Percent complete: 36.2%; Average loss: 0.0019\n",
            "Iteration: 726; Percent complete: 36.3%; Average loss: 0.0019\n",
            "Iteration: 727; Percent complete: 36.4%; Average loss: 0.0019\n",
            "Iteration: 728; Percent complete: 36.4%; Average loss: 0.0019\n",
            "Iteration: 729; Percent complete: 36.4%; Average loss: 0.0018\n",
            "Iteration: 730; Percent complete: 36.5%; Average loss: 0.0017\n",
            "Iteration: 731; Percent complete: 36.5%; Average loss: 0.0019\n",
            "Iteration: 732; Percent complete: 36.6%; Average loss: 0.0018\n",
            "Iteration: 733; Percent complete: 36.6%; Average loss: 0.0019\n",
            "Iteration: 734; Percent complete: 36.7%; Average loss: 0.0019\n",
            "Iteration: 735; Percent complete: 36.8%; Average loss: 0.0018\n",
            "Iteration: 736; Percent complete: 36.8%; Average loss: 0.0019\n",
            "Iteration: 737; Percent complete: 36.9%; Average loss: 0.0019\n",
            "Iteration: 738; Percent complete: 36.9%; Average loss: 0.0019\n",
            "Iteration: 739; Percent complete: 37.0%; Average loss: 0.0019\n",
            "Iteration: 740; Percent complete: 37.0%; Average loss: 0.0018\n",
            "Iteration: 741; Percent complete: 37.0%; Average loss: 0.0018\n",
            "Iteration: 742; Percent complete: 37.1%; Average loss: 0.0019\n",
            "Iteration: 743; Percent complete: 37.1%; Average loss: 0.0019\n",
            "Iteration: 744; Percent complete: 37.2%; Average loss: 0.0018\n",
            "Iteration: 745; Percent complete: 37.2%; Average loss: 0.0018\n",
            "Iteration: 746; Percent complete: 37.3%; Average loss: 0.0018\n",
            "Iteration: 747; Percent complete: 37.4%; Average loss: 0.0019\n",
            "Iteration: 748; Percent complete: 37.4%; Average loss: 0.0018\n",
            "Iteration: 749; Percent complete: 37.5%; Average loss: 0.0018\n",
            "Iteration: 750; Percent complete: 37.5%; Average loss: 0.0018\n",
            "Iteration: 751; Percent complete: 37.5%; Average loss: 0.0018\n",
            "Iteration: 752; Percent complete: 37.6%; Average loss: 0.0018\n",
            "Iteration: 753; Percent complete: 37.6%; Average loss: 0.0018\n",
            "Iteration: 754; Percent complete: 37.7%; Average loss: 0.0018\n",
            "Iteration: 755; Percent complete: 37.8%; Average loss: 0.0017\n",
            "Iteration: 756; Percent complete: 37.8%; Average loss: 0.0018\n",
            "Iteration: 757; Percent complete: 37.9%; Average loss: 0.0018\n",
            "Iteration: 758; Percent complete: 37.9%; Average loss: 0.0018\n",
            "Iteration: 759; Percent complete: 38.0%; Average loss: 0.0017\n",
            "Iteration: 760; Percent complete: 38.0%; Average loss: 0.0017\n",
            "Iteration: 761; Percent complete: 38.0%; Average loss: 0.0017\n",
            "Iteration: 762; Percent complete: 38.1%; Average loss: 0.0018\n",
            "Iteration: 763; Percent complete: 38.1%; Average loss: 0.0017\n",
            "Iteration: 764; Percent complete: 38.2%; Average loss: 0.0018\n",
            "Iteration: 765; Percent complete: 38.2%; Average loss: 0.0018\n",
            "Iteration: 766; Percent complete: 38.3%; Average loss: 0.0017\n",
            "Iteration: 767; Percent complete: 38.4%; Average loss: 0.0017\n",
            "Iteration: 768; Percent complete: 38.4%; Average loss: 0.0017\n",
            "Iteration: 769; Percent complete: 38.5%; Average loss: 0.0018\n",
            "Iteration: 770; Percent complete: 38.5%; Average loss: 0.0017\n",
            "Iteration: 771; Percent complete: 38.6%; Average loss: 0.0017\n",
            "Iteration: 772; Percent complete: 38.6%; Average loss: 0.0017\n",
            "Iteration: 773; Percent complete: 38.6%; Average loss: 0.0017\n",
            "Iteration: 774; Percent complete: 38.7%; Average loss: 0.0016\n",
            "Iteration: 775; Percent complete: 38.8%; Average loss: 0.0017\n",
            "Iteration: 776; Percent complete: 38.8%; Average loss: 0.0017\n",
            "Iteration: 777; Percent complete: 38.9%; Average loss: 0.0017\n",
            "Iteration: 778; Percent complete: 38.9%; Average loss: 0.0017\n",
            "Iteration: 779; Percent complete: 39.0%; Average loss: 0.0017\n",
            "Iteration: 780; Percent complete: 39.0%; Average loss: 0.0017\n",
            "Iteration: 781; Percent complete: 39.1%; Average loss: 0.0017\n",
            "Iteration: 782; Percent complete: 39.1%; Average loss: 0.0017\n",
            "Iteration: 783; Percent complete: 39.1%; Average loss: 0.0017\n",
            "Iteration: 784; Percent complete: 39.2%; Average loss: 0.0016\n",
            "Iteration: 785; Percent complete: 39.2%; Average loss: 0.0016\n",
            "Iteration: 786; Percent complete: 39.3%; Average loss: 0.0017\n",
            "Iteration: 787; Percent complete: 39.4%; Average loss: 0.0017\n",
            "Iteration: 788; Percent complete: 39.4%; Average loss: 0.0016\n",
            "Iteration: 789; Percent complete: 39.5%; Average loss: 0.0016\n",
            "Iteration: 790; Percent complete: 39.5%; Average loss: 0.0016\n",
            "Iteration: 791; Percent complete: 39.6%; Average loss: 0.0016\n",
            "Iteration: 792; Percent complete: 39.6%; Average loss: 0.0016\n",
            "Iteration: 793; Percent complete: 39.6%; Average loss: 0.0017\n",
            "Iteration: 794; Percent complete: 39.7%; Average loss: 0.0016\n",
            "Iteration: 795; Percent complete: 39.8%; Average loss: 0.0016\n",
            "Iteration: 796; Percent complete: 39.8%; Average loss: 0.0017\n",
            "Iteration: 797; Percent complete: 39.9%; Average loss: 0.0016\n",
            "Iteration: 798; Percent complete: 39.9%; Average loss: 0.0015\n",
            "Iteration: 799; Percent complete: 40.0%; Average loss: 0.0017\n",
            "Iteration: 800; Percent complete: 40.0%; Average loss: 0.0016\n",
            "Iteration: 801; Percent complete: 40.1%; Average loss: 0.0016\n",
            "Iteration: 802; Percent complete: 40.1%; Average loss: 0.0016\n",
            "Iteration: 803; Percent complete: 40.2%; Average loss: 0.0016\n",
            "Iteration: 804; Percent complete: 40.2%; Average loss: 0.0016\n",
            "Iteration: 805; Percent complete: 40.2%; Average loss: 0.0175\n",
            "Iteration: 806; Percent complete: 40.3%; Average loss: 0.0016\n",
            "Iteration: 807; Percent complete: 40.4%; Average loss: 0.0017\n",
            "Iteration: 808; Percent complete: 40.4%; Average loss: 0.0039\n",
            "Iteration: 809; Percent complete: 40.5%; Average loss: 0.0054\n",
            "Iteration: 810; Percent complete: 40.5%; Average loss: 0.0055\n",
            "Iteration: 811; Percent complete: 40.6%; Average loss: 0.0070\n",
            "Iteration: 812; Percent complete: 40.6%; Average loss: 0.0054\n",
            "Iteration: 813; Percent complete: 40.6%; Average loss: 0.0544\n",
            "Iteration: 814; Percent complete: 40.7%; Average loss: 0.0035\n",
            "Iteration: 815; Percent complete: 40.8%; Average loss: 0.0028\n",
            "Iteration: 816; Percent complete: 40.8%; Average loss: 0.0030\n",
            "Iteration: 817; Percent complete: 40.8%; Average loss: 0.0030\n",
            "Iteration: 818; Percent complete: 40.9%; Average loss: 0.0028\n",
            "Iteration: 819; Percent complete: 40.9%; Average loss: 0.0029\n",
            "Iteration: 820; Percent complete: 41.0%; Average loss: 0.0026\n",
            "Iteration: 821; Percent complete: 41.0%; Average loss: 0.0027\n",
            "Iteration: 822; Percent complete: 41.1%; Average loss: 0.0030\n",
            "Iteration: 823; Percent complete: 41.1%; Average loss: 0.0030\n",
            "Iteration: 824; Percent complete: 41.2%; Average loss: 0.0032\n",
            "Iteration: 825; Percent complete: 41.2%; Average loss: 0.0033\n",
            "Iteration: 826; Percent complete: 41.3%; Average loss: 0.0032\n",
            "Iteration: 827; Percent complete: 41.3%; Average loss: 0.0030\n",
            "Iteration: 828; Percent complete: 41.4%; Average loss: 0.0032\n",
            "Iteration: 829; Percent complete: 41.4%; Average loss: 0.0029\n",
            "Iteration: 830; Percent complete: 41.5%; Average loss: 0.0062\n",
            "Iteration: 831; Percent complete: 41.5%; Average loss: 0.0033\n",
            "Iteration: 832; Percent complete: 41.6%; Average loss: 0.0026\n",
            "Iteration: 833; Percent complete: 41.6%; Average loss: 0.0031\n",
            "Iteration: 834; Percent complete: 41.7%; Average loss: 0.0040\n",
            "Iteration: 835; Percent complete: 41.8%; Average loss: 0.0026\n",
            "Iteration: 836; Percent complete: 41.8%; Average loss: 0.0028\n",
            "Iteration: 837; Percent complete: 41.9%; Average loss: 0.0024\n",
            "Iteration: 838; Percent complete: 41.9%; Average loss: 0.0029\n",
            "Iteration: 839; Percent complete: 41.9%; Average loss: 0.0205\n",
            "Iteration: 840; Percent complete: 42.0%; Average loss: 0.0037\n",
            "Iteration: 841; Percent complete: 42.0%; Average loss: 0.0024\n",
            "Iteration: 842; Percent complete: 42.1%; Average loss: 0.0236\n",
            "Iteration: 843; Percent complete: 42.1%; Average loss: 0.0038\n",
            "Iteration: 844; Percent complete: 42.2%; Average loss: 0.0041\n",
            "Iteration: 845; Percent complete: 42.2%; Average loss: 0.0029\n",
            "Iteration: 846; Percent complete: 42.3%; Average loss: 0.0042\n",
            "Iteration: 847; Percent complete: 42.4%; Average loss: 0.0052\n",
            "Iteration: 848; Percent complete: 42.4%; Average loss: 0.0063\n",
            "Iteration: 849; Percent complete: 42.4%; Average loss: 0.0086\n",
            "Iteration: 850; Percent complete: 42.5%; Average loss: 0.0046\n",
            "Iteration: 851; Percent complete: 42.5%; Average loss: 0.0082\n",
            "Iteration: 852; Percent complete: 42.6%; Average loss: 0.0079\n",
            "Iteration: 853; Percent complete: 42.6%; Average loss: 0.0077\n",
            "Iteration: 854; Percent complete: 42.7%; Average loss: 0.0063\n",
            "Iteration: 855; Percent complete: 42.8%; Average loss: 0.0046\n",
            "Iteration: 856; Percent complete: 42.8%; Average loss: 0.0038\n",
            "Iteration: 857; Percent complete: 42.9%; Average loss: 0.0052\n",
            "Iteration: 858; Percent complete: 42.9%; Average loss: 0.0036\n",
            "Iteration: 859; Percent complete: 43.0%; Average loss: 0.0052\n",
            "Iteration: 860; Percent complete: 43.0%; Average loss: 0.0038\n",
            "Iteration: 861; Percent complete: 43.0%; Average loss: 0.0050\n",
            "Iteration: 862; Percent complete: 43.1%; Average loss: 0.0040\n",
            "Iteration: 863; Percent complete: 43.1%; Average loss: 0.0035\n",
            "Iteration: 864; Percent complete: 43.2%; Average loss: 0.0040\n",
            "Iteration: 865; Percent complete: 43.2%; Average loss: 0.0041\n",
            "Iteration: 866; Percent complete: 43.3%; Average loss: 0.0036\n",
            "Iteration: 867; Percent complete: 43.4%; Average loss: 0.0035\n",
            "Iteration: 868; Percent complete: 43.4%; Average loss: 0.0037\n",
            "Iteration: 869; Percent complete: 43.5%; Average loss: 0.0036\n",
            "Iteration: 870; Percent complete: 43.5%; Average loss: 0.0034\n",
            "Iteration: 871; Percent complete: 43.5%; Average loss: 0.0036\n",
            "Iteration: 872; Percent complete: 43.6%; Average loss: 0.0029\n",
            "Iteration: 873; Percent complete: 43.6%; Average loss: 0.0030\n",
            "Iteration: 874; Percent complete: 43.7%; Average loss: 0.0029\n",
            "Iteration: 875; Percent complete: 43.8%; Average loss: 0.0031\n",
            "Iteration: 876; Percent complete: 43.8%; Average loss: 0.0029\n",
            "Iteration: 877; Percent complete: 43.9%; Average loss: 0.0032\n",
            "Iteration: 878; Percent complete: 43.9%; Average loss: 0.0030\n",
            "Iteration: 879; Percent complete: 44.0%; Average loss: 0.0027\n",
            "Iteration: 880; Percent complete: 44.0%; Average loss: 0.0026\n",
            "Iteration: 881; Percent complete: 44.0%; Average loss: 0.0028\n",
            "Iteration: 882; Percent complete: 44.1%; Average loss: 0.0027\n",
            "Iteration: 883; Percent complete: 44.1%; Average loss: 0.0026\n",
            "Iteration: 884; Percent complete: 44.2%; Average loss: 0.0026\n",
            "Iteration: 885; Percent complete: 44.2%; Average loss: 0.0028\n",
            "Iteration: 886; Percent complete: 44.3%; Average loss: 0.0026\n",
            "Iteration: 887; Percent complete: 44.4%; Average loss: 0.0025\n",
            "Iteration: 888; Percent complete: 44.4%; Average loss: 0.0027\n",
            "Iteration: 889; Percent complete: 44.5%; Average loss: 0.0025\n",
            "Iteration: 890; Percent complete: 44.5%; Average loss: 0.0025\n",
            "Iteration: 891; Percent complete: 44.5%; Average loss: 0.0023\n",
            "Iteration: 892; Percent complete: 44.6%; Average loss: 0.0026\n",
            "Iteration: 893; Percent complete: 44.6%; Average loss: 0.0024\n",
            "Iteration: 894; Percent complete: 44.7%; Average loss: 0.0024\n",
            "Iteration: 895; Percent complete: 44.8%; Average loss: 0.0031\n",
            "Iteration: 896; Percent complete: 44.8%; Average loss: 0.0028\n",
            "Iteration: 897; Percent complete: 44.9%; Average loss: 0.0023\n",
            "Iteration: 898; Percent complete: 44.9%; Average loss: 0.0023\n",
            "Iteration: 899; Percent complete: 45.0%; Average loss: 0.0024\n",
            "Iteration: 900; Percent complete: 45.0%; Average loss: 0.0024\n",
            "Iteration: 901; Percent complete: 45.1%; Average loss: 0.0024\n",
            "Iteration: 902; Percent complete: 45.1%; Average loss: 0.0023\n",
            "Iteration: 903; Percent complete: 45.1%; Average loss: 0.0023\n",
            "Iteration: 904; Percent complete: 45.2%; Average loss: 0.0023\n",
            "Iteration: 905; Percent complete: 45.2%; Average loss: 0.0021\n",
            "Iteration: 906; Percent complete: 45.3%; Average loss: 0.0021\n",
            "Iteration: 907; Percent complete: 45.4%; Average loss: 0.0022\n",
            "Iteration: 908; Percent complete: 45.4%; Average loss: 0.0022\n",
            "Iteration: 909; Percent complete: 45.5%; Average loss: 0.0020\n",
            "Iteration: 910; Percent complete: 45.5%; Average loss: 0.0021\n",
            "Iteration: 911; Percent complete: 45.6%; Average loss: 0.0021\n",
            "Iteration: 912; Percent complete: 45.6%; Average loss: 0.0020\n",
            "Iteration: 913; Percent complete: 45.6%; Average loss: 0.0021\n",
            "Iteration: 914; Percent complete: 45.7%; Average loss: 0.0021\n",
            "Iteration: 915; Percent complete: 45.8%; Average loss: 0.0021\n",
            "Iteration: 916; Percent complete: 45.8%; Average loss: 0.0022\n",
            "Iteration: 917; Percent complete: 45.9%; Average loss: 0.0021\n",
            "Iteration: 918; Percent complete: 45.9%; Average loss: 0.0021\n",
            "Iteration: 919; Percent complete: 46.0%; Average loss: 0.0021\n",
            "Iteration: 920; Percent complete: 46.0%; Average loss: 0.0020\n",
            "Iteration: 921; Percent complete: 46.1%; Average loss: 0.0021\n",
            "Iteration: 922; Percent complete: 46.1%; Average loss: 0.0020\n",
            "Iteration: 923; Percent complete: 46.2%; Average loss: 0.0019\n",
            "Iteration: 924; Percent complete: 46.2%; Average loss: 0.0019\n",
            "Iteration: 925; Percent complete: 46.2%; Average loss: 0.0021\n",
            "Iteration: 926; Percent complete: 46.3%; Average loss: 0.0020\n",
            "Iteration: 927; Percent complete: 46.4%; Average loss: 0.0020\n",
            "Iteration: 928; Percent complete: 46.4%; Average loss: 0.0019\n",
            "Iteration: 929; Percent complete: 46.5%; Average loss: 0.0019\n",
            "Iteration: 930; Percent complete: 46.5%; Average loss: 0.0020\n",
            "Iteration: 931; Percent complete: 46.6%; Average loss: 0.0019\n",
            "Iteration: 932; Percent complete: 46.6%; Average loss: 0.0019\n",
            "Iteration: 933; Percent complete: 46.7%; Average loss: 0.0020\n",
            "Iteration: 934; Percent complete: 46.7%; Average loss: 0.0019\n",
            "Iteration: 935; Percent complete: 46.8%; Average loss: 0.0019\n",
            "Iteration: 936; Percent complete: 46.8%; Average loss: 0.0019\n",
            "Iteration: 937; Percent complete: 46.9%; Average loss: 0.0019\n",
            "Iteration: 938; Percent complete: 46.9%; Average loss: 0.0018\n",
            "Iteration: 939; Percent complete: 46.9%; Average loss: 0.0018\n",
            "Iteration: 940; Percent complete: 47.0%; Average loss: 0.0019\n",
            "Iteration: 941; Percent complete: 47.0%; Average loss: 0.0017\n",
            "Iteration: 942; Percent complete: 47.1%; Average loss: 0.0018\n",
            "Iteration: 943; Percent complete: 47.1%; Average loss: 0.0018\n",
            "Iteration: 944; Percent complete: 47.2%; Average loss: 0.0018\n",
            "Iteration: 945; Percent complete: 47.2%; Average loss: 0.0018\n",
            "Iteration: 946; Percent complete: 47.3%; Average loss: 0.0018\n",
            "Iteration: 947; Percent complete: 47.3%; Average loss: 0.0017\n",
            "Iteration: 948; Percent complete: 47.4%; Average loss: 0.0018\n",
            "Iteration: 949; Percent complete: 47.4%; Average loss: 0.0019\n",
            "Iteration: 950; Percent complete: 47.5%; Average loss: 0.0018\n",
            "Iteration: 951; Percent complete: 47.5%; Average loss: 0.0017\n",
            "Iteration: 952; Percent complete: 47.6%; Average loss: 0.0017\n",
            "Iteration: 953; Percent complete: 47.6%; Average loss: 0.0016\n",
            "Iteration: 954; Percent complete: 47.7%; Average loss: 0.0017\n",
            "Iteration: 955; Percent complete: 47.8%; Average loss: 0.0018\n",
            "Iteration: 956; Percent complete: 47.8%; Average loss: 0.0017\n",
            "Iteration: 957; Percent complete: 47.9%; Average loss: 0.0018\n",
            "Iteration: 958; Percent complete: 47.9%; Average loss: 0.0017\n",
            "Iteration: 959; Percent complete: 47.9%; Average loss: 0.0017\n",
            "Iteration: 960; Percent complete: 48.0%; Average loss: 0.0018\n",
            "Iteration: 961; Percent complete: 48.0%; Average loss: 0.0017\n",
            "Iteration: 962; Percent complete: 48.1%; Average loss: 0.0017\n",
            "Iteration: 963; Percent complete: 48.1%; Average loss: 0.0017\n",
            "Iteration: 964; Percent complete: 48.2%; Average loss: 0.0016\n",
            "Iteration: 965; Percent complete: 48.2%; Average loss: 0.0015\n",
            "Iteration: 966; Percent complete: 48.3%; Average loss: 0.0017\n",
            "Iteration: 967; Percent complete: 48.4%; Average loss: 0.0016\n",
            "Iteration: 968; Percent complete: 48.4%; Average loss: 0.0016\n",
            "Iteration: 969; Percent complete: 48.4%; Average loss: 0.0017\n",
            "Iteration: 970; Percent complete: 48.5%; Average loss: 0.0016\n",
            "Iteration: 971; Percent complete: 48.5%; Average loss: 0.0016\n",
            "Iteration: 972; Percent complete: 48.6%; Average loss: 0.0016\n",
            "Iteration: 973; Percent complete: 48.6%; Average loss: 0.0017\n",
            "Iteration: 974; Percent complete: 48.7%; Average loss: 0.0017\n",
            "Iteration: 975; Percent complete: 48.8%; Average loss: 0.0016\n",
            "Iteration: 976; Percent complete: 48.8%; Average loss: 0.0016\n",
            "Iteration: 977; Percent complete: 48.9%; Average loss: 0.0016\n",
            "Iteration: 978; Percent complete: 48.9%; Average loss: 0.0016\n",
            "Iteration: 979; Percent complete: 48.9%; Average loss: 0.0016\n",
            "Iteration: 980; Percent complete: 49.0%; Average loss: 0.0016\n",
            "Iteration: 981; Percent complete: 49.0%; Average loss: 0.0016\n",
            "Iteration: 982; Percent complete: 49.1%; Average loss: 0.0016\n",
            "Iteration: 983; Percent complete: 49.1%; Average loss: 0.0017\n",
            "Iteration: 984; Percent complete: 49.2%; Average loss: 0.0015\n",
            "Iteration: 985; Percent complete: 49.2%; Average loss: 0.0015\n",
            "Iteration: 986; Percent complete: 49.3%; Average loss: 0.0015\n",
            "Iteration: 987; Percent complete: 49.4%; Average loss: 0.0015\n",
            "Iteration: 988; Percent complete: 49.4%; Average loss: 0.0016\n",
            "Iteration: 989; Percent complete: 49.5%; Average loss: 0.0015\n",
            "Iteration: 990; Percent complete: 49.5%; Average loss: 0.0015\n",
            "Iteration: 991; Percent complete: 49.5%; Average loss: 0.0015\n",
            "Iteration: 992; Percent complete: 49.6%; Average loss: 0.0015\n",
            "Iteration: 993; Percent complete: 49.6%; Average loss: 0.0016\n",
            "Iteration: 994; Percent complete: 49.7%; Average loss: 0.0015\n",
            "Iteration: 995; Percent complete: 49.8%; Average loss: 0.0016\n",
            "Iteration: 996; Percent complete: 49.8%; Average loss: 0.0015\n",
            "Iteration: 997; Percent complete: 49.9%; Average loss: 0.0015\n",
            "Iteration: 998; Percent complete: 49.9%; Average loss: 0.0014\n",
            "Iteration: 999; Percent complete: 50.0%; Average loss: 0.0015\n",
            "Iteration: 1000; Percent complete: 50.0%; Average loss: 0.0014\n",
            "Iteration: 1001; Percent complete: 50.0%; Average loss: 0.0014\n",
            "Iteration: 1002; Percent complete: 50.1%; Average loss: 0.0015\n",
            "Iteration: 1003; Percent complete: 50.1%; Average loss: 0.0015\n",
            "Iteration: 1004; Percent complete: 50.2%; Average loss: 0.0015\n",
            "Iteration: 1005; Percent complete: 50.2%; Average loss: 0.0015\n",
            "Iteration: 1006; Percent complete: 50.3%; Average loss: 0.0014\n",
            "Iteration: 1007; Percent complete: 50.3%; Average loss: 0.0015\n",
            "Iteration: 1008; Percent complete: 50.4%; Average loss: 0.0014\n",
            "Iteration: 1009; Percent complete: 50.4%; Average loss: 0.0014\n",
            "Iteration: 1010; Percent complete: 50.5%; Average loss: 0.0014\n",
            "Iteration: 1011; Percent complete: 50.5%; Average loss: 0.0013\n",
            "Iteration: 1012; Percent complete: 50.6%; Average loss: 0.0014\n",
            "Iteration: 1013; Percent complete: 50.6%; Average loss: 0.0015\n",
            "Iteration: 1014; Percent complete: 50.7%; Average loss: 0.0014\n",
            "Iteration: 1015; Percent complete: 50.7%; Average loss: 0.0014\n",
            "Iteration: 1016; Percent complete: 50.8%; Average loss: 0.0014\n",
            "Iteration: 1017; Percent complete: 50.8%; Average loss: 0.0014\n",
            "Iteration: 1018; Percent complete: 50.9%; Average loss: 0.0014\n",
            "Iteration: 1019; Percent complete: 50.9%; Average loss: 0.0014\n",
            "Iteration: 1020; Percent complete: 51.0%; Average loss: 0.0013\n",
            "Iteration: 1021; Percent complete: 51.0%; Average loss: 0.0014\n",
            "Iteration: 1022; Percent complete: 51.1%; Average loss: 0.0014\n",
            "Iteration: 1023; Percent complete: 51.1%; Average loss: 0.0014\n",
            "Iteration: 1024; Percent complete: 51.2%; Average loss: 0.0015\n",
            "Iteration: 1025; Percent complete: 51.2%; Average loss: 0.0014\n",
            "Iteration: 1026; Percent complete: 51.3%; Average loss: 0.0015\n",
            "Iteration: 1027; Percent complete: 51.3%; Average loss: 0.0014\n",
            "Iteration: 1028; Percent complete: 51.4%; Average loss: 0.0013\n",
            "Iteration: 1029; Percent complete: 51.4%; Average loss: 0.0013\n",
            "Iteration: 1030; Percent complete: 51.5%; Average loss: 0.0014\n",
            "Iteration: 1031; Percent complete: 51.5%; Average loss: 0.0013\n",
            "Iteration: 1032; Percent complete: 51.6%; Average loss: 0.0013\n",
            "Iteration: 1033; Percent complete: 51.6%; Average loss: 0.0013\n",
            "Iteration: 1034; Percent complete: 51.7%; Average loss: 0.0013\n",
            "Iteration: 1035; Percent complete: 51.7%; Average loss: 0.0013\n",
            "Iteration: 1036; Percent complete: 51.8%; Average loss: 0.0014\n",
            "Iteration: 1037; Percent complete: 51.8%; Average loss: 0.0013\n",
            "Iteration: 1038; Percent complete: 51.9%; Average loss: 0.0013\n",
            "Iteration: 1039; Percent complete: 51.9%; Average loss: 0.0014\n",
            "Iteration: 1040; Percent complete: 52.0%; Average loss: 0.0012\n",
            "Iteration: 1041; Percent complete: 52.0%; Average loss: 0.0012\n",
            "Iteration: 1042; Percent complete: 52.1%; Average loss: 0.0013\n",
            "Iteration: 1043; Percent complete: 52.1%; Average loss: 0.0013\n",
            "Iteration: 1044; Percent complete: 52.2%; Average loss: 0.0013\n",
            "Iteration: 1045; Percent complete: 52.2%; Average loss: 0.0013\n",
            "Iteration: 1046; Percent complete: 52.3%; Average loss: 0.0014\n",
            "Iteration: 1047; Percent complete: 52.3%; Average loss: 0.0013\n",
            "Iteration: 1048; Percent complete: 52.4%; Average loss: 0.0014\n",
            "Iteration: 1049; Percent complete: 52.4%; Average loss: 0.0012\n",
            "Iteration: 1050; Percent complete: 52.5%; Average loss: 0.0013\n",
            "Iteration: 1051; Percent complete: 52.5%; Average loss: 0.0012\n",
            "Iteration: 1052; Percent complete: 52.6%; Average loss: 0.0013\n",
            "Iteration: 1053; Percent complete: 52.6%; Average loss: 0.0012\n",
            "Iteration: 1054; Percent complete: 52.7%; Average loss: 0.0012\n",
            "Iteration: 1055; Percent complete: 52.8%; Average loss: 0.0013\n",
            "Iteration: 1056; Percent complete: 52.8%; Average loss: 0.0012\n",
            "Iteration: 1057; Percent complete: 52.8%; Average loss: 0.0013\n",
            "Iteration: 1058; Percent complete: 52.9%; Average loss: 0.0013\n",
            "Iteration: 1059; Percent complete: 52.9%; Average loss: 0.0012\n",
            "Iteration: 1060; Percent complete: 53.0%; Average loss: 0.0012\n",
            "Iteration: 1061; Percent complete: 53.0%; Average loss: 0.0012\n",
            "Iteration: 1062; Percent complete: 53.1%; Average loss: 0.0013\n",
            "Iteration: 1063; Percent complete: 53.1%; Average loss: 0.0013\n",
            "Iteration: 1064; Percent complete: 53.2%; Average loss: 0.0012\n",
            "Iteration: 1065; Percent complete: 53.2%; Average loss: 0.0013\n",
            "Iteration: 1066; Percent complete: 53.3%; Average loss: 0.0012\n",
            "Iteration: 1067; Percent complete: 53.3%; Average loss: 0.0012\n",
            "Iteration: 1068; Percent complete: 53.4%; Average loss: 0.0012\n",
            "Iteration: 1069; Percent complete: 53.4%; Average loss: 0.0013\n",
            "Iteration: 1070; Percent complete: 53.5%; Average loss: 0.0013\n",
            "Iteration: 1071; Percent complete: 53.5%; Average loss: 0.0012\n",
            "Iteration: 1072; Percent complete: 53.6%; Average loss: 0.0012\n",
            "Iteration: 1073; Percent complete: 53.6%; Average loss: 0.0012\n",
            "Iteration: 1074; Percent complete: 53.7%; Average loss: 0.0012\n",
            "Iteration: 1075; Percent complete: 53.8%; Average loss: 0.0012\n",
            "Iteration: 1076; Percent complete: 53.8%; Average loss: 0.0012\n",
            "Iteration: 1077; Percent complete: 53.8%; Average loss: 0.0012\n",
            "Iteration: 1078; Percent complete: 53.9%; Average loss: 0.0012\n",
            "Iteration: 1079; Percent complete: 53.9%; Average loss: 0.0012\n",
            "Iteration: 1080; Percent complete: 54.0%; Average loss: 0.0012\n",
            "Iteration: 1081; Percent complete: 54.0%; Average loss: 0.0011\n",
            "Iteration: 1082; Percent complete: 54.1%; Average loss: 0.0013\n",
            "Iteration: 1083; Percent complete: 54.1%; Average loss: 0.0012\n",
            "Iteration: 1084; Percent complete: 54.2%; Average loss: 0.0012\n",
            "Iteration: 1085; Percent complete: 54.2%; Average loss: 0.0012\n",
            "Iteration: 1086; Percent complete: 54.3%; Average loss: 0.0012\n",
            "Iteration: 1087; Percent complete: 54.4%; Average loss: 0.0011\n",
            "Iteration: 1088; Percent complete: 54.4%; Average loss: 0.0012\n",
            "Iteration: 1089; Percent complete: 54.4%; Average loss: 0.0011\n",
            "Iteration: 1090; Percent complete: 54.5%; Average loss: 0.0012\n",
            "Iteration: 1091; Percent complete: 54.5%; Average loss: 0.0012\n",
            "Iteration: 1092; Percent complete: 54.6%; Average loss: 0.0011\n",
            "Iteration: 1093; Percent complete: 54.6%; Average loss: 0.0012\n",
            "Iteration: 1094; Percent complete: 54.7%; Average loss: 0.0011\n",
            "Iteration: 1095; Percent complete: 54.8%; Average loss: 0.0011\n",
            "Iteration: 1096; Percent complete: 54.8%; Average loss: 0.0011\n",
            "Iteration: 1097; Percent complete: 54.9%; Average loss: 0.0011\n",
            "Iteration: 1098; Percent complete: 54.9%; Average loss: 0.0011\n",
            "Iteration: 1099; Percent complete: 54.9%; Average loss: 0.0011\n",
            "Iteration: 1100; Percent complete: 55.0%; Average loss: 0.0011\n",
            "Iteration: 1101; Percent complete: 55.0%; Average loss: 0.0011\n",
            "Iteration: 1102; Percent complete: 55.1%; Average loss: 0.0011\n",
            "Iteration: 1103; Percent complete: 55.1%; Average loss: 0.0012\n",
            "Iteration: 1104; Percent complete: 55.2%; Average loss: 0.0011\n",
            "Iteration: 1105; Percent complete: 55.2%; Average loss: 0.0011\n",
            "Iteration: 1106; Percent complete: 55.3%; Average loss: 0.0011\n",
            "Iteration: 1107; Percent complete: 55.4%; Average loss: 0.0011\n",
            "Iteration: 1108; Percent complete: 55.4%; Average loss: 0.0011\n",
            "Iteration: 1109; Percent complete: 55.5%; Average loss: 0.0011\n",
            "Iteration: 1110; Percent complete: 55.5%; Average loss: 0.0011\n",
            "Iteration: 1111; Percent complete: 55.5%; Average loss: 0.0011\n",
            "Iteration: 1112; Percent complete: 55.6%; Average loss: 0.0011\n",
            "Iteration: 1113; Percent complete: 55.6%; Average loss: 0.0011\n",
            "Iteration: 1114; Percent complete: 55.7%; Average loss: 0.0011\n",
            "Iteration: 1115; Percent complete: 55.8%; Average loss: 0.0012\n",
            "Iteration: 1116; Percent complete: 55.8%; Average loss: 0.0010\n",
            "Iteration: 1117; Percent complete: 55.9%; Average loss: 0.0011\n",
            "Iteration: 1118; Percent complete: 55.9%; Average loss: 0.0011\n",
            "Iteration: 1119; Percent complete: 56.0%; Average loss: 0.0011\n",
            "Iteration: 1120; Percent complete: 56.0%; Average loss: 0.0011\n",
            "Iteration: 1121; Percent complete: 56.0%; Average loss: 0.0011\n",
            "Iteration: 1122; Percent complete: 56.1%; Average loss: 0.0011\n",
            "Iteration: 1123; Percent complete: 56.1%; Average loss: 0.0011\n",
            "Iteration: 1124; Percent complete: 56.2%; Average loss: 0.0010\n",
            "Iteration: 1125; Percent complete: 56.2%; Average loss: 0.0010\n",
            "Iteration: 1126; Percent complete: 56.3%; Average loss: 0.0011\n",
            "Iteration: 1127; Percent complete: 56.4%; Average loss: 0.0011\n",
            "Iteration: 1128; Percent complete: 56.4%; Average loss: 0.0012\n",
            "Iteration: 1129; Percent complete: 56.5%; Average loss: 0.0010\n",
            "Iteration: 1130; Percent complete: 56.5%; Average loss: 0.0011\n",
            "Iteration: 1131; Percent complete: 56.5%; Average loss: 0.0011\n",
            "Iteration: 1132; Percent complete: 56.6%; Average loss: 0.0010\n",
            "Iteration: 1133; Percent complete: 56.6%; Average loss: 0.0011\n",
            "Iteration: 1134; Percent complete: 56.7%; Average loss: 0.0010\n",
            "Iteration: 1135; Percent complete: 56.8%; Average loss: 0.0010\n",
            "Iteration: 1136; Percent complete: 56.8%; Average loss: 0.0010\n",
            "Iteration: 1137; Percent complete: 56.9%; Average loss: 0.0010\n",
            "Iteration: 1138; Percent complete: 56.9%; Average loss: 0.0010\n",
            "Iteration: 1139; Percent complete: 57.0%; Average loss: 0.0010\n",
            "Iteration: 1140; Percent complete: 57.0%; Average loss: 0.0010\n",
            "Iteration: 1141; Percent complete: 57.0%; Average loss: 0.0010\n",
            "Iteration: 1142; Percent complete: 57.1%; Average loss: 0.0010\n",
            "Iteration: 1143; Percent complete: 57.1%; Average loss: 0.0010\n",
            "Iteration: 1144; Percent complete: 57.2%; Average loss: 0.0010\n",
            "Iteration: 1145; Percent complete: 57.2%; Average loss: 0.0010\n",
            "Iteration: 1146; Percent complete: 57.3%; Average loss: 0.0010\n",
            "Iteration: 1147; Percent complete: 57.4%; Average loss: 0.0010\n",
            "Iteration: 1148; Percent complete: 57.4%; Average loss: 0.0010\n",
            "Iteration: 1149; Percent complete: 57.5%; Average loss: 0.0011\n",
            "Iteration: 1150; Percent complete: 57.5%; Average loss: 0.0010\n",
            "Iteration: 1151; Percent complete: 57.6%; Average loss: 0.0010\n",
            "Iteration: 1152; Percent complete: 57.6%; Average loss: 0.0010\n",
            "Iteration: 1153; Percent complete: 57.6%; Average loss: 0.0010\n",
            "Iteration: 1154; Percent complete: 57.7%; Average loss: 0.0010\n",
            "Iteration: 1155; Percent complete: 57.8%; Average loss: 0.0010\n",
            "Iteration: 1156; Percent complete: 57.8%; Average loss: 0.0010\n",
            "Iteration: 1157; Percent complete: 57.9%; Average loss: 0.0010\n",
            "Iteration: 1158; Percent complete: 57.9%; Average loss: 0.0010\n",
            "Iteration: 1159; Percent complete: 58.0%; Average loss: 0.0010\n",
            "Iteration: 1160; Percent complete: 58.0%; Average loss: 0.0010\n",
            "Iteration: 1161; Percent complete: 58.1%; Average loss: 0.0010\n",
            "Iteration: 1162; Percent complete: 58.1%; Average loss: 0.0010\n",
            "Iteration: 1163; Percent complete: 58.1%; Average loss: 0.0010\n",
            "Iteration: 1164; Percent complete: 58.2%; Average loss: 0.0010\n",
            "Iteration: 1165; Percent complete: 58.2%; Average loss: 0.0010\n",
            "Iteration: 1166; Percent complete: 58.3%; Average loss: 0.0009\n",
            "Iteration: 1167; Percent complete: 58.4%; Average loss: 0.0010\n",
            "Iteration: 1168; Percent complete: 58.4%; Average loss: 0.0010\n",
            "Iteration: 1169; Percent complete: 58.5%; Average loss: 0.0010\n",
            "Iteration: 1170; Percent complete: 58.5%; Average loss: 0.0009\n",
            "Iteration: 1171; Percent complete: 58.6%; Average loss: 0.0009\n",
            "Iteration: 1172; Percent complete: 58.6%; Average loss: 0.0009\n",
            "Iteration: 1173; Percent complete: 58.7%; Average loss: 0.0010\n",
            "Iteration: 1174; Percent complete: 58.7%; Average loss: 0.0010\n",
            "Iteration: 1175; Percent complete: 58.8%; Average loss: 0.0010\n",
            "Iteration: 1176; Percent complete: 58.8%; Average loss: 0.0009\n",
            "Iteration: 1177; Percent complete: 58.9%; Average loss: 0.0009\n",
            "Iteration: 1178; Percent complete: 58.9%; Average loss: 0.0009\n",
            "Iteration: 1179; Percent complete: 59.0%; Average loss: 0.0009\n",
            "Iteration: 1180; Percent complete: 59.0%; Average loss: 0.0010\n",
            "Iteration: 1181; Percent complete: 59.1%; Average loss: 0.0009\n",
            "Iteration: 1182; Percent complete: 59.1%; Average loss: 0.0009\n",
            "Iteration: 1183; Percent complete: 59.2%; Average loss: 0.0009\n",
            "Iteration: 1184; Percent complete: 59.2%; Average loss: 0.0010\n",
            "Iteration: 1185; Percent complete: 59.2%; Average loss: 0.0010\n",
            "Iteration: 1186; Percent complete: 59.3%; Average loss: 0.0010\n",
            "Iteration: 1187; Percent complete: 59.4%; Average loss: 0.0009\n",
            "Iteration: 1188; Percent complete: 59.4%; Average loss: 0.0009\n",
            "Iteration: 1189; Percent complete: 59.5%; Average loss: 0.0009\n",
            "Iteration: 1190; Percent complete: 59.5%; Average loss: 0.0009\n",
            "Iteration: 1191; Percent complete: 59.6%; Average loss: 0.0009\n",
            "Iteration: 1192; Percent complete: 59.6%; Average loss: 0.0009\n",
            "Iteration: 1193; Percent complete: 59.7%; Average loss: 0.0009\n",
            "Iteration: 1194; Percent complete: 59.7%; Average loss: 0.0009\n",
            "Iteration: 1195; Percent complete: 59.8%; Average loss: 0.0009\n",
            "Iteration: 1196; Percent complete: 59.8%; Average loss: 0.0009\n",
            "Iteration: 1197; Percent complete: 59.9%; Average loss: 0.0009\n",
            "Iteration: 1198; Percent complete: 59.9%; Average loss: 0.0009\n",
            "Iteration: 1199; Percent complete: 60.0%; Average loss: 0.0009\n",
            "Iteration: 1200; Percent complete: 60.0%; Average loss: 0.0009\n",
            "Iteration: 1201; Percent complete: 60.1%; Average loss: 0.0009\n",
            "Iteration: 1202; Percent complete: 60.1%; Average loss: 0.0009\n",
            "Iteration: 1203; Percent complete: 60.2%; Average loss: 0.0009\n",
            "Iteration: 1204; Percent complete: 60.2%; Average loss: 0.0009\n",
            "Iteration: 1205; Percent complete: 60.2%; Average loss: 0.0009\n",
            "Iteration: 1206; Percent complete: 60.3%; Average loss: 0.0009\n",
            "Iteration: 1207; Percent complete: 60.4%; Average loss: 0.0008\n",
            "Iteration: 1208; Percent complete: 60.4%; Average loss: 0.0009\n",
            "Iteration: 1209; Percent complete: 60.5%; Average loss: 0.0009\n",
            "Iteration: 1210; Percent complete: 60.5%; Average loss: 0.0009\n",
            "Iteration: 1211; Percent complete: 60.6%; Average loss: 0.0009\n",
            "Iteration: 1212; Percent complete: 60.6%; Average loss: 0.0009\n",
            "Iteration: 1213; Percent complete: 60.7%; Average loss: 0.0009\n",
            "Iteration: 1214; Percent complete: 60.7%; Average loss: 0.0009\n",
            "Iteration: 1215; Percent complete: 60.8%; Average loss: 0.0009\n",
            "Iteration: 1216; Percent complete: 60.8%; Average loss: 0.0009\n",
            "Iteration: 1217; Percent complete: 60.9%; Average loss: 0.0009\n",
            "Iteration: 1218; Percent complete: 60.9%; Average loss: 0.0009\n",
            "Iteration: 1219; Percent complete: 61.0%; Average loss: 0.0009\n",
            "Iteration: 1220; Percent complete: 61.0%; Average loss: 0.0008\n",
            "Iteration: 1221; Percent complete: 61.1%; Average loss: 0.0009\n",
            "Iteration: 1222; Percent complete: 61.1%; Average loss: 0.0008\n",
            "Iteration: 1223; Percent complete: 61.2%; Average loss: 0.0009\n",
            "Iteration: 1224; Percent complete: 61.2%; Average loss: 0.0008\n",
            "Iteration: 1225; Percent complete: 61.3%; Average loss: 0.0009\n",
            "Iteration: 1226; Percent complete: 61.3%; Average loss: 0.0009\n",
            "Iteration: 1227; Percent complete: 61.4%; Average loss: 0.0009\n",
            "Iteration: 1228; Percent complete: 61.4%; Average loss: 0.0008\n",
            "Iteration: 1229; Percent complete: 61.5%; Average loss: 0.0009\n",
            "Iteration: 1230; Percent complete: 61.5%; Average loss: 0.0009\n",
            "Iteration: 1231; Percent complete: 61.6%; Average loss: 0.0009\n",
            "Iteration: 1232; Percent complete: 61.6%; Average loss: 0.0008\n",
            "Iteration: 1233; Percent complete: 61.7%; Average loss: 0.0009\n",
            "Iteration: 1234; Percent complete: 61.7%; Average loss: 0.0008\n",
            "Iteration: 1235; Percent complete: 61.8%; Average loss: 0.0008\n",
            "Iteration: 1236; Percent complete: 61.8%; Average loss: 0.0008\n",
            "Iteration: 1237; Percent complete: 61.9%; Average loss: 0.0009\n",
            "Iteration: 1238; Percent complete: 61.9%; Average loss: 0.0009\n",
            "Iteration: 1239; Percent complete: 62.0%; Average loss: 0.0009\n",
            "Iteration: 1240; Percent complete: 62.0%; Average loss: 0.0008\n",
            "Iteration: 1241; Percent complete: 62.1%; Average loss: 0.0008\n",
            "Iteration: 1242; Percent complete: 62.1%; Average loss: 0.0009\n",
            "Iteration: 1243; Percent complete: 62.2%; Average loss: 0.0008\n",
            "Iteration: 1244; Percent complete: 62.2%; Average loss: 0.0008\n",
            "Iteration: 1245; Percent complete: 62.3%; Average loss: 0.0008\n",
            "Iteration: 1246; Percent complete: 62.3%; Average loss: 0.0008\n",
            "Iteration: 1247; Percent complete: 62.4%; Average loss: 0.0009\n",
            "Iteration: 1248; Percent complete: 62.4%; Average loss: 0.0008\n",
            "Iteration: 1249; Percent complete: 62.5%; Average loss: 0.0009\n",
            "Iteration: 1250; Percent complete: 62.5%; Average loss: 0.0008\n",
            "Iteration: 1251; Percent complete: 62.5%; Average loss: 0.0008\n",
            "Iteration: 1252; Percent complete: 62.6%; Average loss: 0.0008\n",
            "Iteration: 1253; Percent complete: 62.6%; Average loss: 0.0008\n",
            "Iteration: 1254; Percent complete: 62.7%; Average loss: 0.0008\n",
            "Iteration: 1255; Percent complete: 62.7%; Average loss: 0.0009\n",
            "Iteration: 1256; Percent complete: 62.8%; Average loss: 0.0008\n",
            "Iteration: 1257; Percent complete: 62.8%; Average loss: 0.0008\n",
            "Iteration: 1258; Percent complete: 62.9%; Average loss: 0.0008\n",
            "Iteration: 1259; Percent complete: 62.9%; Average loss: 0.0008\n",
            "Iteration: 1260; Percent complete: 63.0%; Average loss: 0.0008\n",
            "Iteration: 1261; Percent complete: 63.0%; Average loss: 0.0008\n",
            "Iteration: 1262; Percent complete: 63.1%; Average loss: 0.0008\n",
            "Iteration: 1263; Percent complete: 63.1%; Average loss: 0.0008\n",
            "Iteration: 1264; Percent complete: 63.2%; Average loss: 0.0008\n",
            "Iteration: 1265; Percent complete: 63.2%; Average loss: 0.0008\n",
            "Iteration: 1266; Percent complete: 63.3%; Average loss: 0.0008\n",
            "Iteration: 1267; Percent complete: 63.3%; Average loss: 0.0008\n",
            "Iteration: 1268; Percent complete: 63.4%; Average loss: 0.0008\n",
            "Iteration: 1269; Percent complete: 63.4%; Average loss: 0.0008\n",
            "Iteration: 1270; Percent complete: 63.5%; Average loss: 0.0008\n",
            "Iteration: 1271; Percent complete: 63.5%; Average loss: 0.0008\n",
            "Iteration: 1272; Percent complete: 63.6%; Average loss: 0.0008\n",
            "Iteration: 1273; Percent complete: 63.6%; Average loss: 0.0008\n",
            "Iteration: 1274; Percent complete: 63.7%; Average loss: 0.0008\n",
            "Iteration: 1275; Percent complete: 63.7%; Average loss: 0.0008\n",
            "Iteration: 1276; Percent complete: 63.8%; Average loss: 0.0008\n",
            "Iteration: 1277; Percent complete: 63.8%; Average loss: 0.0008\n",
            "Iteration: 1278; Percent complete: 63.9%; Average loss: 0.0008\n",
            "Iteration: 1279; Percent complete: 63.9%; Average loss: 0.0008\n",
            "Iteration: 1280; Percent complete: 64.0%; Average loss: 0.0008\n",
            "Iteration: 1281; Percent complete: 64.0%; Average loss: 0.0008\n",
            "Iteration: 1282; Percent complete: 64.1%; Average loss: 0.0008\n",
            "Iteration: 1283; Percent complete: 64.1%; Average loss: 0.0008\n",
            "Iteration: 1284; Percent complete: 64.2%; Average loss: 0.0008\n",
            "Iteration: 1285; Percent complete: 64.2%; Average loss: 0.0008\n",
            "Iteration: 1286; Percent complete: 64.3%; Average loss: 0.0008\n",
            "Iteration: 1287; Percent complete: 64.3%; Average loss: 0.0008\n",
            "Iteration: 1288; Percent complete: 64.4%; Average loss: 0.0008\n",
            "Iteration: 1289; Percent complete: 64.5%; Average loss: 0.0008\n",
            "Iteration: 1290; Percent complete: 64.5%; Average loss: 0.0008\n",
            "Iteration: 1291; Percent complete: 64.5%; Average loss: 0.0008\n",
            "Iteration: 1292; Percent complete: 64.6%; Average loss: 0.0008\n",
            "Iteration: 1293; Percent complete: 64.6%; Average loss: 0.0008\n",
            "Iteration: 1294; Percent complete: 64.7%; Average loss: 0.0007\n",
            "Iteration: 1295; Percent complete: 64.8%; Average loss: 0.0008\n",
            "Iteration: 1296; Percent complete: 64.8%; Average loss: 0.0008\n",
            "Iteration: 1297; Percent complete: 64.8%; Average loss: 0.0008\n",
            "Iteration: 1298; Percent complete: 64.9%; Average loss: 0.0008\n",
            "Iteration: 1299; Percent complete: 65.0%; Average loss: 0.0007\n",
            "Iteration: 1300; Percent complete: 65.0%; Average loss: 0.0008\n",
            "Iteration: 1301; Percent complete: 65.0%; Average loss: 0.0007\n",
            "Iteration: 1302; Percent complete: 65.1%; Average loss: 0.0007\n",
            "Iteration: 1303; Percent complete: 65.1%; Average loss: 0.0008\n",
            "Iteration: 1304; Percent complete: 65.2%; Average loss: 0.0008\n",
            "Iteration: 1305; Percent complete: 65.2%; Average loss: 0.0008\n",
            "Iteration: 1306; Percent complete: 65.3%; Average loss: 0.0007\n",
            "Iteration: 1307; Percent complete: 65.3%; Average loss: 0.0008\n",
            "Iteration: 1308; Percent complete: 65.4%; Average loss: 0.0007\n",
            "Iteration: 1309; Percent complete: 65.5%; Average loss: 0.0007\n",
            "Iteration: 1310; Percent complete: 65.5%; Average loss: 0.0007\n",
            "Iteration: 1311; Percent complete: 65.5%; Average loss: 0.0007\n",
            "Iteration: 1312; Percent complete: 65.6%; Average loss: 0.0007\n",
            "Iteration: 1313; Percent complete: 65.6%; Average loss: 0.0007\n",
            "Iteration: 1314; Percent complete: 65.7%; Average loss: 0.0007\n",
            "Iteration: 1315; Percent complete: 65.8%; Average loss: 0.0008\n",
            "Iteration: 1316; Percent complete: 65.8%; Average loss: 0.0007\n",
            "Iteration: 1317; Percent complete: 65.8%; Average loss: 0.0008\n",
            "Iteration: 1318; Percent complete: 65.9%; Average loss: 0.0007\n",
            "Iteration: 1319; Percent complete: 66.0%; Average loss: 0.0007\n",
            "Iteration: 1320; Percent complete: 66.0%; Average loss: 0.0007\n",
            "Iteration: 1321; Percent complete: 66.0%; Average loss: 0.0007\n",
            "Iteration: 1322; Percent complete: 66.1%; Average loss: 0.0007\n",
            "Iteration: 1323; Percent complete: 66.1%; Average loss: 0.0008\n",
            "Iteration: 1324; Percent complete: 66.2%; Average loss: 0.0007\n",
            "Iteration: 1325; Percent complete: 66.2%; Average loss: 0.0007\n",
            "Iteration: 1326; Percent complete: 66.3%; Average loss: 0.0007\n",
            "Iteration: 1327; Percent complete: 66.3%; Average loss: 0.0007\n",
            "Iteration: 1328; Percent complete: 66.4%; Average loss: 0.0007\n",
            "Iteration: 1329; Percent complete: 66.5%; Average loss: 0.0007\n",
            "Iteration: 1330; Percent complete: 66.5%; Average loss: 0.0007\n",
            "Iteration: 1331; Percent complete: 66.5%; Average loss: 0.0007\n",
            "Iteration: 1332; Percent complete: 66.6%; Average loss: 0.0007\n",
            "Iteration: 1333; Percent complete: 66.6%; Average loss: 0.0007\n",
            "Iteration: 1334; Percent complete: 66.7%; Average loss: 0.0008\n",
            "Iteration: 1335; Percent complete: 66.8%; Average loss: 0.0007\n",
            "Iteration: 1336; Percent complete: 66.8%; Average loss: 0.0007\n",
            "Iteration: 1337; Percent complete: 66.8%; Average loss: 0.0007\n",
            "Iteration: 1338; Percent complete: 66.9%; Average loss: 0.0007\n",
            "Iteration: 1339; Percent complete: 67.0%; Average loss: 0.0007\n",
            "Iteration: 1340; Percent complete: 67.0%; Average loss: 0.0007\n",
            "Iteration: 1341; Percent complete: 67.0%; Average loss: 0.0007\n",
            "Iteration: 1342; Percent complete: 67.1%; Average loss: 0.0007\n",
            "Iteration: 1343; Percent complete: 67.2%; Average loss: 0.0007\n",
            "Iteration: 1344; Percent complete: 67.2%; Average loss: 0.0007\n",
            "Iteration: 1345; Percent complete: 67.2%; Average loss: 0.0007\n",
            "Iteration: 1346; Percent complete: 67.3%; Average loss: 0.0007\n",
            "Iteration: 1347; Percent complete: 67.3%; Average loss: 0.0007\n",
            "Iteration: 1348; Percent complete: 67.4%; Average loss: 0.0007\n",
            "Iteration: 1349; Percent complete: 67.5%; Average loss: 0.0007\n",
            "Iteration: 1350; Percent complete: 67.5%; Average loss: 0.0007\n",
            "Iteration: 1351; Percent complete: 67.5%; Average loss: 0.0007\n",
            "Iteration: 1352; Percent complete: 67.6%; Average loss: 0.0007\n",
            "Iteration: 1353; Percent complete: 67.7%; Average loss: 0.0007\n",
            "Iteration: 1354; Percent complete: 67.7%; Average loss: 0.0007\n",
            "Iteration: 1355; Percent complete: 67.8%; Average loss: 0.0007\n",
            "Iteration: 1356; Percent complete: 67.8%; Average loss: 0.0007\n",
            "Iteration: 1357; Percent complete: 67.8%; Average loss: 0.0007\n",
            "Iteration: 1358; Percent complete: 67.9%; Average loss: 0.0007\n",
            "Iteration: 1359; Percent complete: 68.0%; Average loss: 0.0007\n",
            "Iteration: 1360; Percent complete: 68.0%; Average loss: 0.0007\n",
            "Iteration: 1361; Percent complete: 68.0%; Average loss: 0.0007\n",
            "Iteration: 1362; Percent complete: 68.1%; Average loss: 0.0007\n",
            "Iteration: 1363; Percent complete: 68.2%; Average loss: 0.0007\n",
            "Iteration: 1364; Percent complete: 68.2%; Average loss: 0.0007\n",
            "Iteration: 1365; Percent complete: 68.2%; Average loss: 0.0007\n",
            "Iteration: 1366; Percent complete: 68.3%; Average loss: 0.0007\n",
            "Iteration: 1367; Percent complete: 68.3%; Average loss: 0.0006\n",
            "Iteration: 1368; Percent complete: 68.4%; Average loss: 0.0007\n",
            "Iteration: 1369; Percent complete: 68.5%; Average loss: 0.0007\n",
            "Iteration: 1370; Percent complete: 68.5%; Average loss: 0.0007\n",
            "Iteration: 1371; Percent complete: 68.5%; Average loss: 0.0007\n",
            "Iteration: 1372; Percent complete: 68.6%; Average loss: 0.0007\n",
            "Iteration: 1373; Percent complete: 68.7%; Average loss: 0.0007\n",
            "Iteration: 1374; Percent complete: 68.7%; Average loss: 0.0007\n",
            "Iteration: 1375; Percent complete: 68.8%; Average loss: 0.0007\n",
            "Iteration: 1376; Percent complete: 68.8%; Average loss: 0.0007\n",
            "Iteration: 1377; Percent complete: 68.8%; Average loss: 0.0007\n",
            "Iteration: 1378; Percent complete: 68.9%; Average loss: 0.0007\n",
            "Iteration: 1379; Percent complete: 69.0%; Average loss: 0.0007\n",
            "Iteration: 1380; Percent complete: 69.0%; Average loss: 0.0007\n",
            "Iteration: 1381; Percent complete: 69.0%; Average loss: 0.0007\n",
            "Iteration: 1382; Percent complete: 69.1%; Average loss: 0.0007\n",
            "Iteration: 1383; Percent complete: 69.2%; Average loss: 0.0007\n",
            "Iteration: 1384; Percent complete: 69.2%; Average loss: 0.0006\n",
            "Iteration: 1385; Percent complete: 69.2%; Average loss: 0.0007\n",
            "Iteration: 1386; Percent complete: 69.3%; Average loss: 0.0006\n",
            "Iteration: 1387; Percent complete: 69.3%; Average loss: 0.0007\n",
            "Iteration: 1388; Percent complete: 69.4%; Average loss: 0.0006\n",
            "Iteration: 1389; Percent complete: 69.5%; Average loss: 0.0006\n",
            "Iteration: 1390; Percent complete: 69.5%; Average loss: 0.0006\n",
            "Iteration: 1391; Percent complete: 69.5%; Average loss: 0.0007\n",
            "Iteration: 1392; Percent complete: 69.6%; Average loss: 0.0006\n",
            "Iteration: 1393; Percent complete: 69.7%; Average loss: 0.0006\n",
            "Iteration: 1394; Percent complete: 69.7%; Average loss: 0.0006\n",
            "Iteration: 1395; Percent complete: 69.8%; Average loss: 0.0006\n",
            "Iteration: 1396; Percent complete: 69.8%; Average loss: 0.0006\n",
            "Iteration: 1397; Percent complete: 69.8%; Average loss: 0.0007\n",
            "Iteration: 1398; Percent complete: 69.9%; Average loss: 0.0006\n",
            "Iteration: 1399; Percent complete: 70.0%; Average loss: 0.0006\n",
            "Iteration: 1400; Percent complete: 70.0%; Average loss: 0.0007\n",
            "Iteration: 1401; Percent complete: 70.0%; Average loss: 0.0006\n",
            "Iteration: 1402; Percent complete: 70.1%; Average loss: 0.0007\n",
            "Iteration: 1403; Percent complete: 70.2%; Average loss: 0.0006\n",
            "Iteration: 1404; Percent complete: 70.2%; Average loss: 0.0006\n",
            "Iteration: 1405; Percent complete: 70.2%; Average loss: 0.0006\n",
            "Iteration: 1406; Percent complete: 70.3%; Average loss: 0.0006\n",
            "Iteration: 1407; Percent complete: 70.3%; Average loss: 0.0007\n",
            "Iteration: 1408; Percent complete: 70.4%; Average loss: 0.0006\n",
            "Iteration: 1409; Percent complete: 70.5%; Average loss: 0.0007\n",
            "Iteration: 1410; Percent complete: 70.5%; Average loss: 0.0006\n",
            "Iteration: 1411; Percent complete: 70.5%; Average loss: 0.0006\n",
            "Iteration: 1412; Percent complete: 70.6%; Average loss: 0.0006\n",
            "Iteration: 1413; Percent complete: 70.7%; Average loss: 0.0006\n",
            "Iteration: 1414; Percent complete: 70.7%; Average loss: 0.0006\n",
            "Iteration: 1415; Percent complete: 70.8%; Average loss: 0.0007\n",
            "Iteration: 1416; Percent complete: 70.8%; Average loss: 0.0006\n",
            "Iteration: 1417; Percent complete: 70.9%; Average loss: 0.0007\n",
            "Iteration: 1418; Percent complete: 70.9%; Average loss: 0.0006\n",
            "Iteration: 1419; Percent complete: 71.0%; Average loss: 0.0006\n",
            "Iteration: 1420; Percent complete: 71.0%; Average loss: 0.0006\n",
            "Iteration: 1421; Percent complete: 71.0%; Average loss: 0.0006\n",
            "Iteration: 1422; Percent complete: 71.1%; Average loss: 0.0006\n",
            "Iteration: 1423; Percent complete: 71.2%; Average loss: 0.0006\n",
            "Iteration: 1424; Percent complete: 71.2%; Average loss: 0.0006\n",
            "Iteration: 1425; Percent complete: 71.2%; Average loss: 0.0006\n",
            "Iteration: 1426; Percent complete: 71.3%; Average loss: 0.0006\n",
            "Iteration: 1427; Percent complete: 71.4%; Average loss: 0.0006\n",
            "Iteration: 1428; Percent complete: 71.4%; Average loss: 0.0006\n",
            "Iteration: 1429; Percent complete: 71.5%; Average loss: 0.0006\n",
            "Iteration: 1430; Percent complete: 71.5%; Average loss: 0.0006\n",
            "Iteration: 1431; Percent complete: 71.5%; Average loss: 0.0006\n",
            "Iteration: 1432; Percent complete: 71.6%; Average loss: 0.0006\n",
            "Iteration: 1433; Percent complete: 71.7%; Average loss: 0.0006\n",
            "Iteration: 1434; Percent complete: 71.7%; Average loss: 0.0006\n",
            "Iteration: 1435; Percent complete: 71.8%; Average loss: 0.0006\n",
            "Iteration: 1436; Percent complete: 71.8%; Average loss: 0.0006\n",
            "Iteration: 1437; Percent complete: 71.9%; Average loss: 0.0006\n",
            "Iteration: 1438; Percent complete: 71.9%; Average loss: 0.0006\n",
            "Iteration: 1439; Percent complete: 72.0%; Average loss: 0.0006\n",
            "Iteration: 1440; Percent complete: 72.0%; Average loss: 0.0006\n",
            "Iteration: 1441; Percent complete: 72.0%; Average loss: 0.0006\n",
            "Iteration: 1442; Percent complete: 72.1%; Average loss: 0.0006\n",
            "Iteration: 1443; Percent complete: 72.2%; Average loss: 0.0007\n",
            "Iteration: 1444; Percent complete: 72.2%; Average loss: 0.0006\n",
            "Iteration: 1445; Percent complete: 72.2%; Average loss: 0.0006\n",
            "Iteration: 1446; Percent complete: 72.3%; Average loss: 0.0006\n",
            "Iteration: 1447; Percent complete: 72.4%; Average loss: 0.0006\n",
            "Iteration: 1448; Percent complete: 72.4%; Average loss: 0.0006\n",
            "Iteration: 1449; Percent complete: 72.5%; Average loss: 0.0006\n",
            "Iteration: 1450; Percent complete: 72.5%; Average loss: 0.0006\n",
            "Iteration: 1451; Percent complete: 72.5%; Average loss: 0.0006\n",
            "Iteration: 1452; Percent complete: 72.6%; Average loss: 0.0006\n",
            "Iteration: 1453; Percent complete: 72.7%; Average loss: 0.0006\n",
            "Iteration: 1454; Percent complete: 72.7%; Average loss: 0.0006\n",
            "Iteration: 1455; Percent complete: 72.8%; Average loss: 0.0006\n",
            "Iteration: 1456; Percent complete: 72.8%; Average loss: 0.0006\n",
            "Iteration: 1457; Percent complete: 72.9%; Average loss: 0.0006\n",
            "Iteration: 1458; Percent complete: 72.9%; Average loss: 0.0006\n",
            "Iteration: 1459; Percent complete: 73.0%; Average loss: 0.0006\n",
            "Iteration: 1460; Percent complete: 73.0%; Average loss: 0.0005\n",
            "Iteration: 1461; Percent complete: 73.0%; Average loss: 0.0006\n",
            "Iteration: 1462; Percent complete: 73.1%; Average loss: 0.0006\n",
            "Iteration: 1463; Percent complete: 73.2%; Average loss: 0.0006\n",
            "Iteration: 1464; Percent complete: 73.2%; Average loss: 0.0006\n",
            "Iteration: 1465; Percent complete: 73.2%; Average loss: 0.0005\n",
            "Iteration: 1466; Percent complete: 73.3%; Average loss: 0.0006\n",
            "Iteration: 1467; Percent complete: 73.4%; Average loss: 0.0006\n",
            "Iteration: 1468; Percent complete: 73.4%; Average loss: 0.0006\n",
            "Iteration: 1469; Percent complete: 73.5%; Average loss: 0.0006\n",
            "Iteration: 1470; Percent complete: 73.5%; Average loss: 0.0006\n",
            "Iteration: 1471; Percent complete: 73.6%; Average loss: 0.0006\n",
            "Iteration: 1472; Percent complete: 73.6%; Average loss: 0.0006\n",
            "Iteration: 1473; Percent complete: 73.7%; Average loss: 0.0006\n",
            "Iteration: 1474; Percent complete: 73.7%; Average loss: 0.0006\n",
            "Iteration: 1475; Percent complete: 73.8%; Average loss: 0.0005\n",
            "Iteration: 1476; Percent complete: 73.8%; Average loss: 0.0006\n",
            "Iteration: 1477; Percent complete: 73.9%; Average loss: 0.0006\n",
            "Iteration: 1478; Percent complete: 73.9%; Average loss: 0.0005\n",
            "Iteration: 1479; Percent complete: 74.0%; Average loss: 0.0005\n",
            "Iteration: 1480; Percent complete: 74.0%; Average loss: 0.0006\n",
            "Iteration: 1481; Percent complete: 74.1%; Average loss: 0.0006\n",
            "Iteration: 1482; Percent complete: 74.1%; Average loss: 0.0006\n",
            "Iteration: 1483; Percent complete: 74.2%; Average loss: 0.0006\n",
            "Iteration: 1484; Percent complete: 74.2%; Average loss: 0.0005\n",
            "Iteration: 1485; Percent complete: 74.2%; Average loss: 0.0006\n",
            "Iteration: 1486; Percent complete: 74.3%; Average loss: 0.0006\n",
            "Iteration: 1487; Percent complete: 74.4%; Average loss: 0.0006\n",
            "Iteration: 1488; Percent complete: 74.4%; Average loss: 0.0006\n",
            "Iteration: 1489; Percent complete: 74.5%; Average loss: 0.0006\n",
            "Iteration: 1490; Percent complete: 74.5%; Average loss: 0.0006\n",
            "Iteration: 1491; Percent complete: 74.6%; Average loss: 0.0006\n",
            "Iteration: 1492; Percent complete: 74.6%; Average loss: 0.0006\n",
            "Iteration: 1493; Percent complete: 74.7%; Average loss: 0.0006\n",
            "Iteration: 1494; Percent complete: 74.7%; Average loss: 0.0006\n",
            "Iteration: 1495; Percent complete: 74.8%; Average loss: 0.0006\n",
            "Iteration: 1496; Percent complete: 74.8%; Average loss: 0.0006\n",
            "Iteration: 1497; Percent complete: 74.9%; Average loss: 0.0005\n",
            "Iteration: 1498; Percent complete: 74.9%; Average loss: 0.0006\n",
            "Iteration: 1499; Percent complete: 75.0%; Average loss: 0.0005\n",
            "Iteration: 1500; Percent complete: 75.0%; Average loss: 0.0005\n",
            "Iteration: 1501; Percent complete: 75.0%; Average loss: 0.0006\n",
            "Iteration: 1502; Percent complete: 75.1%; Average loss: 0.0006\n",
            "Iteration: 1503; Percent complete: 75.1%; Average loss: 0.0006\n",
            "Iteration: 1504; Percent complete: 75.2%; Average loss: 0.0006\n",
            "Iteration: 1505; Percent complete: 75.2%; Average loss: 0.0006\n",
            "Iteration: 1506; Percent complete: 75.3%; Average loss: 0.0005\n",
            "Iteration: 1507; Percent complete: 75.3%; Average loss: 0.0005\n",
            "Iteration: 1508; Percent complete: 75.4%; Average loss: 0.0006\n",
            "Iteration: 1509; Percent complete: 75.4%; Average loss: 0.0005\n",
            "Iteration: 1510; Percent complete: 75.5%; Average loss: 0.0005\n",
            "Iteration: 1511; Percent complete: 75.5%; Average loss: 0.0005\n",
            "Iteration: 1512; Percent complete: 75.6%; Average loss: 0.0006\n",
            "Iteration: 1513; Percent complete: 75.6%; Average loss: 0.0005\n",
            "Iteration: 1514; Percent complete: 75.7%; Average loss: 0.0005\n",
            "Iteration: 1515; Percent complete: 75.8%; Average loss: 0.0005\n",
            "Iteration: 1516; Percent complete: 75.8%; Average loss: 0.0005\n",
            "Iteration: 1517; Percent complete: 75.8%; Average loss: 0.0005\n",
            "Iteration: 1518; Percent complete: 75.9%; Average loss: 0.0005\n",
            "Iteration: 1519; Percent complete: 75.9%; Average loss: 0.0005\n",
            "Iteration: 1520; Percent complete: 76.0%; Average loss: 0.0006\n",
            "Iteration: 1521; Percent complete: 76.0%; Average loss: 0.0005\n",
            "Iteration: 1522; Percent complete: 76.1%; Average loss: 0.0005\n",
            "Iteration: 1523; Percent complete: 76.1%; Average loss: 0.0005\n",
            "Iteration: 1524; Percent complete: 76.2%; Average loss: 0.0005\n",
            "Iteration: 1525; Percent complete: 76.2%; Average loss: 0.0005\n",
            "Iteration: 1526; Percent complete: 76.3%; Average loss: 0.0005\n",
            "Iteration: 1527; Percent complete: 76.3%; Average loss: 0.0005\n",
            "Iteration: 1528; Percent complete: 76.4%; Average loss: 0.0005\n",
            "Iteration: 1529; Percent complete: 76.4%; Average loss: 0.0006\n",
            "Iteration: 1530; Percent complete: 76.5%; Average loss: 0.0005\n",
            "Iteration: 1531; Percent complete: 76.5%; Average loss: 0.0005\n",
            "Iteration: 1532; Percent complete: 76.6%; Average loss: 0.0005\n",
            "Iteration: 1533; Percent complete: 76.6%; Average loss: 0.0005\n",
            "Iteration: 1534; Percent complete: 76.7%; Average loss: 0.0005\n",
            "Iteration: 1535; Percent complete: 76.8%; Average loss: 0.0005\n",
            "Iteration: 1536; Percent complete: 76.8%; Average loss: 0.0005\n",
            "Iteration: 1537; Percent complete: 76.8%; Average loss: 0.0005\n",
            "Iteration: 1538; Percent complete: 76.9%; Average loss: 0.0005\n",
            "Iteration: 1539; Percent complete: 77.0%; Average loss: 0.0005\n",
            "Iteration: 1540; Percent complete: 77.0%; Average loss: 0.0005\n",
            "Iteration: 1541; Percent complete: 77.0%; Average loss: 0.0005\n",
            "Iteration: 1542; Percent complete: 77.1%; Average loss: 0.0005\n",
            "Iteration: 1543; Percent complete: 77.1%; Average loss: 0.0005\n",
            "Iteration: 1544; Percent complete: 77.2%; Average loss: 0.0005\n",
            "Iteration: 1545; Percent complete: 77.2%; Average loss: 0.0005\n",
            "Iteration: 1546; Percent complete: 77.3%; Average loss: 0.0005\n",
            "Iteration: 1547; Percent complete: 77.3%; Average loss: 0.0005\n",
            "Iteration: 1548; Percent complete: 77.4%; Average loss: 0.0005\n",
            "Iteration: 1549; Percent complete: 77.5%; Average loss: 0.0005\n",
            "Iteration: 1550; Percent complete: 77.5%; Average loss: 0.0005\n",
            "Iteration: 1551; Percent complete: 77.5%; Average loss: 0.0005\n",
            "Iteration: 1552; Percent complete: 77.6%; Average loss: 0.0005\n",
            "Iteration: 1553; Percent complete: 77.6%; Average loss: 0.0005\n",
            "Iteration: 1554; Percent complete: 77.7%; Average loss: 0.0005\n",
            "Iteration: 1555; Percent complete: 77.8%; Average loss: 0.0005\n",
            "Iteration: 1556; Percent complete: 77.8%; Average loss: 0.0005\n",
            "Iteration: 1557; Percent complete: 77.8%; Average loss: 0.0005\n",
            "Iteration: 1558; Percent complete: 77.9%; Average loss: 0.0005\n",
            "Iteration: 1559; Percent complete: 78.0%; Average loss: 0.0005\n",
            "Iteration: 1560; Percent complete: 78.0%; Average loss: 0.0005\n",
            "Iteration: 1561; Percent complete: 78.0%; Average loss: 0.0005\n",
            "Iteration: 1562; Percent complete: 78.1%; Average loss: 0.0005\n",
            "Iteration: 1563; Percent complete: 78.1%; Average loss: 0.0005\n",
            "Iteration: 1564; Percent complete: 78.2%; Average loss: 0.0005\n",
            "Iteration: 1565; Percent complete: 78.2%; Average loss: 0.0005\n",
            "Iteration: 1566; Percent complete: 78.3%; Average loss: 0.0005\n",
            "Iteration: 1567; Percent complete: 78.3%; Average loss: 0.0005\n",
            "Iteration: 1568; Percent complete: 78.4%; Average loss: 0.0005\n",
            "Iteration: 1569; Percent complete: 78.5%; Average loss: 0.0005\n",
            "Iteration: 1570; Percent complete: 78.5%; Average loss: 0.0005\n",
            "Iteration: 1571; Percent complete: 78.5%; Average loss: 0.0005\n",
            "Iteration: 1572; Percent complete: 78.6%; Average loss: 0.0005\n",
            "Iteration: 1573; Percent complete: 78.6%; Average loss: 0.0005\n",
            "Iteration: 1574; Percent complete: 78.7%; Average loss: 0.0005\n",
            "Iteration: 1575; Percent complete: 78.8%; Average loss: 0.0005\n",
            "Iteration: 1576; Percent complete: 78.8%; Average loss: 0.0005\n",
            "Iteration: 1577; Percent complete: 78.8%; Average loss: 0.0005\n",
            "Iteration: 1578; Percent complete: 78.9%; Average loss: 0.0005\n",
            "Iteration: 1579; Percent complete: 79.0%; Average loss: 0.0005\n",
            "Iteration: 1580; Percent complete: 79.0%; Average loss: 0.0005\n",
            "Iteration: 1581; Percent complete: 79.0%; Average loss: 0.0005\n",
            "Iteration: 1582; Percent complete: 79.1%; Average loss: 0.0005\n",
            "Iteration: 1583; Percent complete: 79.1%; Average loss: 0.0005\n",
            "Iteration: 1584; Percent complete: 79.2%; Average loss: 0.0005\n",
            "Iteration: 1585; Percent complete: 79.2%; Average loss: 0.0005\n",
            "Iteration: 1586; Percent complete: 79.3%; Average loss: 0.0005\n",
            "Iteration: 1587; Percent complete: 79.3%; Average loss: 0.0005\n",
            "Iteration: 1588; Percent complete: 79.4%; Average loss: 0.0005\n",
            "Iteration: 1589; Percent complete: 79.5%; Average loss: 0.0005\n",
            "Iteration: 1590; Percent complete: 79.5%; Average loss: 0.0005\n",
            "Iteration: 1591; Percent complete: 79.5%; Average loss: 0.0005\n",
            "Iteration: 1592; Percent complete: 79.6%; Average loss: 0.0005\n",
            "Iteration: 1593; Percent complete: 79.7%; Average loss: 0.0005\n",
            "Iteration: 1594; Percent complete: 79.7%; Average loss: 0.0005\n",
            "Iteration: 1595; Percent complete: 79.8%; Average loss: 0.0005\n",
            "Iteration: 1596; Percent complete: 79.8%; Average loss: 0.0005\n",
            "Iteration: 1597; Percent complete: 79.8%; Average loss: 0.0005\n",
            "Iteration: 1598; Percent complete: 79.9%; Average loss: 0.0005\n",
            "Iteration: 1599; Percent complete: 80.0%; Average loss: 0.0005\n",
            "Iteration: 1600; Percent complete: 80.0%; Average loss: 0.0005\n",
            "Iteration: 1601; Percent complete: 80.0%; Average loss: 0.0005\n",
            "Iteration: 1602; Percent complete: 80.1%; Average loss: 0.0005\n",
            "Iteration: 1603; Percent complete: 80.2%; Average loss: 0.0005\n",
            "Iteration: 1604; Percent complete: 80.2%; Average loss: 0.0005\n",
            "Iteration: 1605; Percent complete: 80.2%; Average loss: 0.0005\n",
            "Iteration: 1606; Percent complete: 80.3%; Average loss: 0.0005\n",
            "Iteration: 1607; Percent complete: 80.3%; Average loss: 0.0005\n",
            "Iteration: 1608; Percent complete: 80.4%; Average loss: 0.0005\n",
            "Iteration: 1609; Percent complete: 80.5%; Average loss: 0.0004\n",
            "Iteration: 1610; Percent complete: 80.5%; Average loss: 0.0005\n",
            "Iteration: 1611; Percent complete: 80.5%; Average loss: 0.0005\n",
            "Iteration: 1612; Percent complete: 80.6%; Average loss: 0.0005\n",
            "Iteration: 1613; Percent complete: 80.7%; Average loss: 0.0005\n",
            "Iteration: 1614; Percent complete: 80.7%; Average loss: 0.0005\n",
            "Iteration: 1615; Percent complete: 80.8%; Average loss: 0.0005\n",
            "Iteration: 1616; Percent complete: 80.8%; Average loss: 0.0004\n",
            "Iteration: 1617; Percent complete: 80.8%; Average loss: 0.0005\n",
            "Iteration: 1618; Percent complete: 80.9%; Average loss: 0.0005\n",
            "Iteration: 1619; Percent complete: 81.0%; Average loss: 0.0005\n",
            "Iteration: 1620; Percent complete: 81.0%; Average loss: 0.0005\n",
            "Iteration: 1621; Percent complete: 81.0%; Average loss: 0.0005\n",
            "Iteration: 1622; Percent complete: 81.1%; Average loss: 0.0005\n",
            "Iteration: 1623; Percent complete: 81.2%; Average loss: 0.0005\n",
            "Iteration: 1624; Percent complete: 81.2%; Average loss: 0.0004\n",
            "Iteration: 1625; Percent complete: 81.2%; Average loss: 0.0005\n",
            "Iteration: 1626; Percent complete: 81.3%; Average loss: 0.0005\n",
            "Iteration: 1627; Percent complete: 81.3%; Average loss: 0.0005\n",
            "Iteration: 1628; Percent complete: 81.4%; Average loss: 0.0005\n",
            "Iteration: 1629; Percent complete: 81.5%; Average loss: 0.0005\n",
            "Iteration: 1630; Percent complete: 81.5%; Average loss: 0.0005\n",
            "Iteration: 1631; Percent complete: 81.5%; Average loss: 0.0005\n",
            "Iteration: 1632; Percent complete: 81.6%; Average loss: 0.0005\n",
            "Iteration: 1633; Percent complete: 81.7%; Average loss: 0.0005\n",
            "Iteration: 1634; Percent complete: 81.7%; Average loss: 0.0005\n",
            "Iteration: 1635; Percent complete: 81.8%; Average loss: 0.0005\n",
            "Iteration: 1636; Percent complete: 81.8%; Average loss: 0.0005\n",
            "Iteration: 1637; Percent complete: 81.8%; Average loss: 0.0005\n",
            "Iteration: 1638; Percent complete: 81.9%; Average loss: 0.0005\n",
            "Iteration: 1639; Percent complete: 82.0%; Average loss: 0.0004\n",
            "Iteration: 1640; Percent complete: 82.0%; Average loss: 0.0005\n",
            "Iteration: 1641; Percent complete: 82.0%; Average loss: 0.0005\n",
            "Iteration: 1642; Percent complete: 82.1%; Average loss: 0.0005\n",
            "Iteration: 1643; Percent complete: 82.2%; Average loss: 0.0005\n",
            "Iteration: 1644; Percent complete: 82.2%; Average loss: 0.0005\n",
            "Iteration: 1645; Percent complete: 82.2%; Average loss: 0.0005\n",
            "Iteration: 1646; Percent complete: 82.3%; Average loss: 0.0005\n",
            "Iteration: 1647; Percent complete: 82.3%; Average loss: 0.0004\n",
            "Iteration: 1648; Percent complete: 82.4%; Average loss: 0.0005\n",
            "Iteration: 1649; Percent complete: 82.5%; Average loss: 0.0006\n",
            "Iteration: 1650; Percent complete: 82.5%; Average loss: 0.0005\n",
            "Iteration: 1651; Percent complete: 82.5%; Average loss: 0.0005\n",
            "Iteration: 1652; Percent complete: 82.6%; Average loss: 0.0004\n",
            "Iteration: 1653; Percent complete: 82.7%; Average loss: 0.0004\n",
            "Iteration: 1654; Percent complete: 82.7%; Average loss: 0.0004\n",
            "Iteration: 1655; Percent complete: 82.8%; Average loss: 0.0005\n",
            "Iteration: 1656; Percent complete: 82.8%; Average loss: 0.0004\n",
            "Iteration: 1657; Percent complete: 82.8%; Average loss: 0.0004\n",
            "Iteration: 1658; Percent complete: 82.9%; Average loss: 0.0005\n",
            "Iteration: 1659; Percent complete: 83.0%; Average loss: 0.0005\n",
            "Iteration: 1660; Percent complete: 83.0%; Average loss: 0.0004\n",
            "Iteration: 1661; Percent complete: 83.0%; Average loss: 0.0004\n",
            "Iteration: 1662; Percent complete: 83.1%; Average loss: 0.0005\n",
            "Iteration: 1663; Percent complete: 83.2%; Average loss: 0.0004\n",
            "Iteration: 1664; Percent complete: 83.2%; Average loss: 0.0004\n",
            "Iteration: 1665; Percent complete: 83.2%; Average loss: 0.0004\n",
            "Iteration: 1666; Percent complete: 83.3%; Average loss: 0.0004\n",
            "Iteration: 1667; Percent complete: 83.4%; Average loss: 0.0004\n",
            "Iteration: 1668; Percent complete: 83.4%; Average loss: 0.0004\n",
            "Iteration: 1669; Percent complete: 83.5%; Average loss: 0.0004\n",
            "Iteration: 1670; Percent complete: 83.5%; Average loss: 0.0004\n",
            "Iteration: 1671; Percent complete: 83.5%; Average loss: 0.0005\n",
            "Iteration: 1672; Percent complete: 83.6%; Average loss: 0.0005\n",
            "Iteration: 1673; Percent complete: 83.7%; Average loss: 0.0004\n",
            "Iteration: 1674; Percent complete: 83.7%; Average loss: 0.0005\n",
            "Iteration: 1675; Percent complete: 83.8%; Average loss: 0.0005\n",
            "Iteration: 1676; Percent complete: 83.8%; Average loss: 0.0004\n",
            "Iteration: 1677; Percent complete: 83.9%; Average loss: 0.0004\n",
            "Iteration: 1678; Percent complete: 83.9%; Average loss: 0.0004\n",
            "Iteration: 1679; Percent complete: 84.0%; Average loss: 0.0004\n",
            "Iteration: 1680; Percent complete: 84.0%; Average loss: 0.0004\n",
            "Iteration: 1681; Percent complete: 84.0%; Average loss: 0.0005\n",
            "Iteration: 1682; Percent complete: 84.1%; Average loss: 0.0004\n",
            "Iteration: 1683; Percent complete: 84.2%; Average loss: 0.0004\n",
            "Iteration: 1684; Percent complete: 84.2%; Average loss: 0.0004\n",
            "Iteration: 1685; Percent complete: 84.2%; Average loss: 0.0004\n",
            "Iteration: 1686; Percent complete: 84.3%; Average loss: 0.0005\n",
            "Iteration: 1687; Percent complete: 84.4%; Average loss: 0.0004\n",
            "Iteration: 1688; Percent complete: 84.4%; Average loss: 0.0004\n",
            "Iteration: 1689; Percent complete: 84.5%; Average loss: 0.0004\n",
            "Iteration: 1690; Percent complete: 84.5%; Average loss: 0.0004\n",
            "Iteration: 1691; Percent complete: 84.5%; Average loss: 0.0004\n",
            "Iteration: 1692; Percent complete: 84.6%; Average loss: 0.0004\n",
            "Iteration: 1693; Percent complete: 84.7%; Average loss: 0.0004\n",
            "Iteration: 1694; Percent complete: 84.7%; Average loss: 0.0004\n",
            "Iteration: 1695; Percent complete: 84.8%; Average loss: 0.0004\n",
            "Iteration: 1696; Percent complete: 84.8%; Average loss: 0.0004\n",
            "Iteration: 1697; Percent complete: 84.9%; Average loss: 0.0004\n",
            "Iteration: 1698; Percent complete: 84.9%; Average loss: 0.0004\n",
            "Iteration: 1699; Percent complete: 85.0%; Average loss: 0.0004\n",
            "Iteration: 1700; Percent complete: 85.0%; Average loss: 0.0004\n",
            "Iteration: 1701; Percent complete: 85.0%; Average loss: 0.0004\n",
            "Iteration: 1702; Percent complete: 85.1%; Average loss: 0.0004\n",
            "Iteration: 1703; Percent complete: 85.2%; Average loss: 0.0004\n",
            "Iteration: 1704; Percent complete: 85.2%; Average loss: 0.0004\n",
            "Iteration: 1705; Percent complete: 85.2%; Average loss: 0.0004\n",
            "Iteration: 1706; Percent complete: 85.3%; Average loss: 0.0004\n",
            "Iteration: 1707; Percent complete: 85.4%; Average loss: 0.0004\n",
            "Iteration: 1708; Percent complete: 85.4%; Average loss: 0.0005\n",
            "Iteration: 1709; Percent complete: 85.5%; Average loss: 0.0004\n",
            "Iteration: 1710; Percent complete: 85.5%; Average loss: 0.0004\n",
            "Iteration: 1711; Percent complete: 85.5%; Average loss: 0.0004\n",
            "Iteration: 1712; Percent complete: 85.6%; Average loss: 0.0004\n",
            "Iteration: 1713; Percent complete: 85.7%; Average loss: 0.0004\n",
            "Iteration: 1714; Percent complete: 85.7%; Average loss: 0.0004\n",
            "Iteration: 1715; Percent complete: 85.8%; Average loss: 0.0004\n",
            "Iteration: 1716; Percent complete: 85.8%; Average loss: 0.0004\n",
            "Iteration: 1717; Percent complete: 85.9%; Average loss: 0.0004\n",
            "Iteration: 1718; Percent complete: 85.9%; Average loss: 0.0004\n",
            "Iteration: 1719; Percent complete: 86.0%; Average loss: 0.0004\n",
            "Iteration: 1720; Percent complete: 86.0%; Average loss: 0.0004\n",
            "Iteration: 1721; Percent complete: 86.1%; Average loss: 0.0004\n",
            "Iteration: 1722; Percent complete: 86.1%; Average loss: 0.0004\n",
            "Iteration: 1723; Percent complete: 86.2%; Average loss: 0.0004\n",
            "Iteration: 1724; Percent complete: 86.2%; Average loss: 0.0004\n",
            "Iteration: 1725; Percent complete: 86.2%; Average loss: 0.0004\n",
            "Iteration: 1726; Percent complete: 86.3%; Average loss: 0.0004\n",
            "Iteration: 1727; Percent complete: 86.4%; Average loss: 0.0004\n",
            "Iteration: 1728; Percent complete: 86.4%; Average loss: 0.0004\n",
            "Iteration: 1729; Percent complete: 86.5%; Average loss: 0.0004\n",
            "Iteration: 1730; Percent complete: 86.5%; Average loss: 0.0004\n",
            "Iteration: 1731; Percent complete: 86.6%; Average loss: 0.0004\n",
            "Iteration: 1732; Percent complete: 86.6%; Average loss: 0.0004\n",
            "Iteration: 1733; Percent complete: 86.7%; Average loss: 0.0004\n",
            "Iteration: 1734; Percent complete: 86.7%; Average loss: 0.0004\n",
            "Iteration: 1735; Percent complete: 86.8%; Average loss: 0.0004\n",
            "Iteration: 1736; Percent complete: 86.8%; Average loss: 0.0004\n",
            "Iteration: 1737; Percent complete: 86.9%; Average loss: 0.0004\n",
            "Iteration: 1738; Percent complete: 86.9%; Average loss: 0.0004\n",
            "Iteration: 1739; Percent complete: 87.0%; Average loss: 0.0004\n",
            "Iteration: 1740; Percent complete: 87.0%; Average loss: 0.0004\n",
            "Iteration: 1741; Percent complete: 87.1%; Average loss: 0.0004\n",
            "Iteration: 1742; Percent complete: 87.1%; Average loss: 0.0004\n",
            "Iteration: 1743; Percent complete: 87.2%; Average loss: 0.0004\n",
            "Iteration: 1744; Percent complete: 87.2%; Average loss: 0.0004\n",
            "Iteration: 1745; Percent complete: 87.2%; Average loss: 0.0004\n",
            "Iteration: 1746; Percent complete: 87.3%; Average loss: 0.0004\n",
            "Iteration: 1747; Percent complete: 87.4%; Average loss: 0.0004\n",
            "Iteration: 1748; Percent complete: 87.4%; Average loss: 0.0004\n",
            "Iteration: 1749; Percent complete: 87.5%; Average loss: 0.0004\n",
            "Iteration: 1750; Percent complete: 87.5%; Average loss: 0.0004\n",
            "Iteration: 1751; Percent complete: 87.5%; Average loss: 0.0004\n",
            "Iteration: 1752; Percent complete: 87.6%; Average loss: 0.0004\n",
            "Iteration: 1753; Percent complete: 87.6%; Average loss: 0.0004\n",
            "Iteration: 1754; Percent complete: 87.7%; Average loss: 0.0004\n",
            "Iteration: 1755; Percent complete: 87.8%; Average loss: 0.0004\n",
            "Iteration: 1756; Percent complete: 87.8%; Average loss: 0.0004\n",
            "Iteration: 1757; Percent complete: 87.8%; Average loss: 0.0004\n",
            "Iteration: 1758; Percent complete: 87.9%; Average loss: 0.0004\n",
            "Iteration: 1759; Percent complete: 87.9%; Average loss: 0.0004\n",
            "Iteration: 1760; Percent complete: 88.0%; Average loss: 0.0004\n",
            "Iteration: 1761; Percent complete: 88.0%; Average loss: 0.0004\n",
            "Iteration: 1762; Percent complete: 88.1%; Average loss: 0.0004\n",
            "Iteration: 1763; Percent complete: 88.1%; Average loss: 0.0004\n",
            "Iteration: 1764; Percent complete: 88.2%; Average loss: 0.0004\n",
            "Iteration: 1765; Percent complete: 88.2%; Average loss: 0.0004\n",
            "Iteration: 1766; Percent complete: 88.3%; Average loss: 0.0004\n",
            "Iteration: 1767; Percent complete: 88.3%; Average loss: 0.0004\n",
            "Iteration: 1768; Percent complete: 88.4%; Average loss: 0.0004\n",
            "Iteration: 1769; Percent complete: 88.4%; Average loss: 0.0004\n",
            "Iteration: 1770; Percent complete: 88.5%; Average loss: 0.0004\n",
            "Iteration: 1771; Percent complete: 88.5%; Average loss: 0.0004\n",
            "Iteration: 1772; Percent complete: 88.6%; Average loss: 0.0004\n",
            "Iteration: 1773; Percent complete: 88.6%; Average loss: 0.0004\n",
            "Iteration: 1774; Percent complete: 88.7%; Average loss: 0.0004\n",
            "Iteration: 1775; Percent complete: 88.8%; Average loss: 0.0004\n",
            "Iteration: 1776; Percent complete: 88.8%; Average loss: 0.0004\n",
            "Iteration: 1777; Percent complete: 88.8%; Average loss: 0.0004\n",
            "Iteration: 1778; Percent complete: 88.9%; Average loss: 0.0004\n",
            "Iteration: 1779; Percent complete: 88.9%; Average loss: 0.0004\n",
            "Iteration: 1780; Percent complete: 89.0%; Average loss: 0.0004\n",
            "Iteration: 1781; Percent complete: 89.0%; Average loss: 0.0004\n",
            "Iteration: 1782; Percent complete: 89.1%; Average loss: 0.0004\n",
            "Iteration: 1783; Percent complete: 89.1%; Average loss: 0.0004\n",
            "Iteration: 1784; Percent complete: 89.2%; Average loss: 0.0004\n",
            "Iteration: 1785; Percent complete: 89.2%; Average loss: 0.0004\n",
            "Iteration: 1786; Percent complete: 89.3%; Average loss: 0.0004\n",
            "Iteration: 1787; Percent complete: 89.3%; Average loss: 0.0004\n",
            "Iteration: 1788; Percent complete: 89.4%; Average loss: 0.0004\n",
            "Iteration: 1789; Percent complete: 89.5%; Average loss: 0.0004\n",
            "Iteration: 1790; Percent complete: 89.5%; Average loss: 0.0004\n",
            "Iteration: 1791; Percent complete: 89.5%; Average loss: 0.0004\n",
            "Iteration: 1792; Percent complete: 89.6%; Average loss: 0.0004\n",
            "Iteration: 1793; Percent complete: 89.6%; Average loss: 0.0004\n",
            "Iteration: 1794; Percent complete: 89.7%; Average loss: 0.0004\n",
            "Iteration: 1795; Percent complete: 89.8%; Average loss: 0.0004\n",
            "Iteration: 1796; Percent complete: 89.8%; Average loss: 0.0004\n",
            "Iteration: 1797; Percent complete: 89.8%; Average loss: 0.0004\n",
            "Iteration: 1798; Percent complete: 89.9%; Average loss: 0.0004\n",
            "Iteration: 1799; Percent complete: 90.0%; Average loss: 0.0004\n",
            "Iteration: 1800; Percent complete: 90.0%; Average loss: 0.0004\n",
            "Iteration: 1801; Percent complete: 90.0%; Average loss: 0.0004\n",
            "Iteration: 1802; Percent complete: 90.1%; Average loss: 0.0004\n",
            "Iteration: 1803; Percent complete: 90.1%; Average loss: 0.0004\n",
            "Iteration: 1804; Percent complete: 90.2%; Average loss: 0.0004\n",
            "Iteration: 1805; Percent complete: 90.2%; Average loss: 0.0004\n",
            "Iteration: 1806; Percent complete: 90.3%; Average loss: 0.0004\n",
            "Iteration: 1807; Percent complete: 90.3%; Average loss: 0.0004\n",
            "Iteration: 1808; Percent complete: 90.4%; Average loss: 0.0004\n",
            "Iteration: 1809; Percent complete: 90.5%; Average loss: 0.0004\n",
            "Iteration: 1810; Percent complete: 90.5%; Average loss: 0.0004\n",
            "Iteration: 1811; Percent complete: 90.5%; Average loss: 0.0004\n",
            "Iteration: 1812; Percent complete: 90.6%; Average loss: 0.0004\n",
            "Iteration: 1813; Percent complete: 90.6%; Average loss: 0.0004\n",
            "Iteration: 1814; Percent complete: 90.7%; Average loss: 0.0004\n",
            "Iteration: 1815; Percent complete: 90.8%; Average loss: 0.0004\n",
            "Iteration: 1816; Percent complete: 90.8%; Average loss: 0.0004\n",
            "Iteration: 1817; Percent complete: 90.8%; Average loss: 0.0004\n",
            "Iteration: 1818; Percent complete: 90.9%; Average loss: 0.0004\n",
            "Iteration: 1819; Percent complete: 91.0%; Average loss: 0.0004\n",
            "Iteration: 1820; Percent complete: 91.0%; Average loss: 0.0004\n",
            "Iteration: 1821; Percent complete: 91.0%; Average loss: 0.0004\n",
            "Iteration: 1822; Percent complete: 91.1%; Average loss: 0.0004\n",
            "Iteration: 1823; Percent complete: 91.1%; Average loss: 0.0004\n",
            "Iteration: 1824; Percent complete: 91.2%; Average loss: 0.0004\n",
            "Iteration: 1825; Percent complete: 91.2%; Average loss: 0.0004\n",
            "Iteration: 1826; Percent complete: 91.3%; Average loss: 0.0004\n",
            "Iteration: 1827; Percent complete: 91.3%; Average loss: 0.0003\n",
            "Iteration: 1828; Percent complete: 91.4%; Average loss: 0.0004\n",
            "Iteration: 1829; Percent complete: 91.5%; Average loss: 0.0004\n",
            "Iteration: 1830; Percent complete: 91.5%; Average loss: 0.0004\n",
            "Iteration: 1831; Percent complete: 91.5%; Average loss: 0.0004\n",
            "Iteration: 1832; Percent complete: 91.6%; Average loss: 0.0004\n",
            "Iteration: 1833; Percent complete: 91.6%; Average loss: 0.0004\n",
            "Iteration: 1834; Percent complete: 91.7%; Average loss: 0.0004\n",
            "Iteration: 1835; Percent complete: 91.8%; Average loss: 0.0004\n",
            "Iteration: 1836; Percent complete: 91.8%; Average loss: 0.0004\n",
            "Iteration: 1837; Percent complete: 91.8%; Average loss: 0.0004\n",
            "Iteration: 1838; Percent complete: 91.9%; Average loss: 0.0004\n",
            "Iteration: 1839; Percent complete: 92.0%; Average loss: 0.0004\n",
            "Iteration: 1840; Percent complete: 92.0%; Average loss: 0.0004\n",
            "Iteration: 1841; Percent complete: 92.0%; Average loss: 0.0003\n",
            "Iteration: 1842; Percent complete: 92.1%; Average loss: 0.0004\n",
            "Iteration: 1843; Percent complete: 92.2%; Average loss: 0.0004\n",
            "Iteration: 1844; Percent complete: 92.2%; Average loss: 0.0004\n",
            "Iteration: 1845; Percent complete: 92.2%; Average loss: 0.0003\n",
            "Iteration: 1846; Percent complete: 92.3%; Average loss: 0.0004\n",
            "Iteration: 1847; Percent complete: 92.3%; Average loss: 0.0004\n",
            "Iteration: 1848; Percent complete: 92.4%; Average loss: 0.0004\n",
            "Iteration: 1849; Percent complete: 92.5%; Average loss: 0.0003\n",
            "Iteration: 1850; Percent complete: 92.5%; Average loss: 0.0004\n",
            "Iteration: 1851; Percent complete: 92.5%; Average loss: 0.0003\n",
            "Iteration: 1852; Percent complete: 92.6%; Average loss: 0.0004\n",
            "Iteration: 1853; Percent complete: 92.7%; Average loss: 0.0003\n",
            "Iteration: 1854; Percent complete: 92.7%; Average loss: 0.0004\n",
            "Iteration: 1855; Percent complete: 92.8%; Average loss: 0.0004\n",
            "Iteration: 1856; Percent complete: 92.8%; Average loss: 0.0003\n",
            "Iteration: 1857; Percent complete: 92.8%; Average loss: 0.0004\n",
            "Iteration: 1858; Percent complete: 92.9%; Average loss: 0.0003\n",
            "Iteration: 1859; Percent complete: 93.0%; Average loss: 0.0004\n",
            "Iteration: 1860; Percent complete: 93.0%; Average loss: 0.0004\n",
            "Iteration: 1861; Percent complete: 93.0%; Average loss: 0.0004\n",
            "Iteration: 1862; Percent complete: 93.1%; Average loss: 0.0004\n",
            "Iteration: 1863; Percent complete: 93.2%; Average loss: 0.0003\n",
            "Iteration: 1864; Percent complete: 93.2%; Average loss: 0.0003\n",
            "Iteration: 1865; Percent complete: 93.2%; Average loss: 0.0003\n",
            "Iteration: 1866; Percent complete: 93.3%; Average loss: 0.0003\n",
            "Iteration: 1867; Percent complete: 93.3%; Average loss: 0.0003\n",
            "Iteration: 1868; Percent complete: 93.4%; Average loss: 0.0004\n",
            "Iteration: 1869; Percent complete: 93.5%; Average loss: 0.0003\n",
            "Iteration: 1870; Percent complete: 93.5%; Average loss: 0.0003\n",
            "Iteration: 1871; Percent complete: 93.5%; Average loss: 0.0003\n",
            "Iteration: 1872; Percent complete: 93.6%; Average loss: 0.0003\n",
            "Iteration: 1873; Percent complete: 93.7%; Average loss: 0.0003\n",
            "Iteration: 1874; Percent complete: 93.7%; Average loss: 0.0003\n",
            "Iteration: 1875; Percent complete: 93.8%; Average loss: 0.0003\n",
            "Iteration: 1876; Percent complete: 93.8%; Average loss: 0.0003\n",
            "Iteration: 1877; Percent complete: 93.8%; Average loss: 0.0003\n",
            "Iteration: 1878; Percent complete: 93.9%; Average loss: 0.0004\n",
            "Iteration: 1879; Percent complete: 94.0%; Average loss: 0.0004\n",
            "Iteration: 1880; Percent complete: 94.0%; Average loss: 0.0004\n",
            "Iteration: 1881; Percent complete: 94.0%; Average loss: 0.0004\n",
            "Iteration: 1882; Percent complete: 94.1%; Average loss: 0.0004\n",
            "Iteration: 1883; Percent complete: 94.2%; Average loss: 0.0004\n",
            "Iteration: 1884; Percent complete: 94.2%; Average loss: 0.0003\n",
            "Iteration: 1885; Percent complete: 94.2%; Average loss: 0.0004\n",
            "Iteration: 1886; Percent complete: 94.3%; Average loss: 0.0003\n",
            "Iteration: 1887; Percent complete: 94.3%; Average loss: 0.0004\n",
            "Iteration: 1888; Percent complete: 94.4%; Average loss: 0.0003\n",
            "Iteration: 1889; Percent complete: 94.5%; Average loss: 0.0003\n",
            "Iteration: 1890; Percent complete: 94.5%; Average loss: 0.0003\n",
            "Iteration: 1891; Percent complete: 94.5%; Average loss: 0.0003\n",
            "Iteration: 1892; Percent complete: 94.6%; Average loss: 0.0003\n",
            "Iteration: 1893; Percent complete: 94.7%; Average loss: 0.0003\n",
            "Iteration: 1894; Percent complete: 94.7%; Average loss: 0.0003\n",
            "Iteration: 1895; Percent complete: 94.8%; Average loss: 0.0003\n",
            "Iteration: 1896; Percent complete: 94.8%; Average loss: 0.0003\n",
            "Iteration: 1897; Percent complete: 94.8%; Average loss: 0.0003\n",
            "Iteration: 1898; Percent complete: 94.9%; Average loss: 0.0003\n",
            "Iteration: 1899; Percent complete: 95.0%; Average loss: 0.0003\n",
            "Iteration: 1900; Percent complete: 95.0%; Average loss: 0.0003\n",
            "Iteration: 1901; Percent complete: 95.0%; Average loss: 0.0003\n",
            "Iteration: 1902; Percent complete: 95.1%; Average loss: 0.0003\n",
            "Iteration: 1903; Percent complete: 95.2%; Average loss: 0.0003\n",
            "Iteration: 1904; Percent complete: 95.2%; Average loss: 0.0003\n",
            "Iteration: 1905; Percent complete: 95.2%; Average loss: 0.0003\n",
            "Iteration: 1906; Percent complete: 95.3%; Average loss: 0.0003\n",
            "Iteration: 1907; Percent complete: 95.3%; Average loss: 0.0003\n",
            "Iteration: 1908; Percent complete: 95.4%; Average loss: 0.0003\n",
            "Iteration: 1909; Percent complete: 95.5%; Average loss: 0.0003\n",
            "Iteration: 1910; Percent complete: 95.5%; Average loss: 0.0003\n",
            "Iteration: 1911; Percent complete: 95.5%; Average loss: 0.0003\n",
            "Iteration: 1912; Percent complete: 95.6%; Average loss: 0.0003\n",
            "Iteration: 1913; Percent complete: 95.7%; Average loss: 0.0003\n",
            "Iteration: 1914; Percent complete: 95.7%; Average loss: 0.0003\n",
            "Iteration: 1915; Percent complete: 95.8%; Average loss: 0.0003\n",
            "Iteration: 1916; Percent complete: 95.8%; Average loss: 0.0003\n",
            "Iteration: 1917; Percent complete: 95.9%; Average loss: 0.0003\n",
            "Iteration: 1918; Percent complete: 95.9%; Average loss: 0.0003\n",
            "Iteration: 1919; Percent complete: 96.0%; Average loss: 0.0003\n",
            "Iteration: 1920; Percent complete: 96.0%; Average loss: 0.0003\n",
            "Iteration: 1921; Percent complete: 96.0%; Average loss: 0.0003\n",
            "Iteration: 1922; Percent complete: 96.1%; Average loss: 0.0003\n",
            "Iteration: 1923; Percent complete: 96.2%; Average loss: 0.0003\n",
            "Iteration: 1924; Percent complete: 96.2%; Average loss: 0.0003\n",
            "Iteration: 1925; Percent complete: 96.2%; Average loss: 0.0003\n",
            "Iteration: 1926; Percent complete: 96.3%; Average loss: 0.0003\n",
            "Iteration: 1927; Percent complete: 96.4%; Average loss: 0.0003\n",
            "Iteration: 1928; Percent complete: 96.4%; Average loss: 0.0003\n",
            "Iteration: 1929; Percent complete: 96.5%; Average loss: 0.0003\n",
            "Iteration: 1930; Percent complete: 96.5%; Average loss: 0.0003\n",
            "Iteration: 1931; Percent complete: 96.5%; Average loss: 0.0003\n",
            "Iteration: 1932; Percent complete: 96.6%; Average loss: 0.0003\n",
            "Iteration: 1933; Percent complete: 96.7%; Average loss: 0.0003\n",
            "Iteration: 1934; Percent complete: 96.7%; Average loss: 0.0003\n",
            "Iteration: 1935; Percent complete: 96.8%; Average loss: 0.0003\n",
            "Iteration: 1936; Percent complete: 96.8%; Average loss: 0.0003\n",
            "Iteration: 1937; Percent complete: 96.9%; Average loss: 0.0003\n",
            "Iteration: 1938; Percent complete: 96.9%; Average loss: 0.0003\n",
            "Iteration: 1939; Percent complete: 97.0%; Average loss: 0.0003\n",
            "Iteration: 1940; Percent complete: 97.0%; Average loss: 0.0003\n",
            "Iteration: 1941; Percent complete: 97.0%; Average loss: 0.0003\n",
            "Iteration: 1942; Percent complete: 97.1%; Average loss: 0.0003\n",
            "Iteration: 1943; Percent complete: 97.2%; Average loss: 0.0003\n",
            "Iteration: 1944; Percent complete: 97.2%; Average loss: 0.0003\n",
            "Iteration: 1945; Percent complete: 97.2%; Average loss: 0.0003\n",
            "Iteration: 1946; Percent complete: 97.3%; Average loss: 0.0003\n",
            "Iteration: 1947; Percent complete: 97.4%; Average loss: 0.0003\n",
            "Iteration: 1948; Percent complete: 97.4%; Average loss: 0.0003\n",
            "Iteration: 1949; Percent complete: 97.5%; Average loss: 0.0003\n",
            "Iteration: 1950; Percent complete: 97.5%; Average loss: 0.0003\n",
            "Iteration: 1951; Percent complete: 97.5%; Average loss: 0.0003\n",
            "Iteration: 1952; Percent complete: 97.6%; Average loss: 0.0003\n",
            "Iteration: 1953; Percent complete: 97.7%; Average loss: 0.0003\n",
            "Iteration: 1954; Percent complete: 97.7%; Average loss: 0.0003\n",
            "Iteration: 1955; Percent complete: 97.8%; Average loss: 0.0003\n",
            "Iteration: 1956; Percent complete: 97.8%; Average loss: 0.0003\n",
            "Iteration: 1957; Percent complete: 97.9%; Average loss: 0.0003\n",
            "Iteration: 1958; Percent complete: 97.9%; Average loss: 0.0003\n",
            "Iteration: 1959; Percent complete: 98.0%; Average loss: 0.0003\n",
            "Iteration: 1960; Percent complete: 98.0%; Average loss: 0.0003\n",
            "Iteration: 1961; Percent complete: 98.0%; Average loss: 0.0003\n",
            "Iteration: 1962; Percent complete: 98.1%; Average loss: 0.0003\n",
            "Iteration: 1963; Percent complete: 98.2%; Average loss: 0.0003\n",
            "Iteration: 1964; Percent complete: 98.2%; Average loss: 0.0003\n",
            "Iteration: 1965; Percent complete: 98.2%; Average loss: 0.0003\n",
            "Iteration: 1966; Percent complete: 98.3%; Average loss: 0.0003\n",
            "Iteration: 1967; Percent complete: 98.4%; Average loss: 0.0003\n",
            "Iteration: 1968; Percent complete: 98.4%; Average loss: 0.0003\n",
            "Iteration: 1969; Percent complete: 98.5%; Average loss: 0.0003\n",
            "Iteration: 1970; Percent complete: 98.5%; Average loss: 0.0003\n",
            "Iteration: 1971; Percent complete: 98.6%; Average loss: 0.0003\n",
            "Iteration: 1972; Percent complete: 98.6%; Average loss: 0.0003\n",
            "Iteration: 1973; Percent complete: 98.7%; Average loss: 0.0003\n",
            "Iteration: 1974; Percent complete: 98.7%; Average loss: 0.0003\n",
            "Iteration: 1975; Percent complete: 98.8%; Average loss: 0.0003\n",
            "Iteration: 1976; Percent complete: 98.8%; Average loss: 0.0003\n",
            "Iteration: 1977; Percent complete: 98.9%; Average loss: 0.0003\n",
            "Iteration: 1978; Percent complete: 98.9%; Average loss: 0.0003\n",
            "Iteration: 1979; Percent complete: 99.0%; Average loss: 0.0003\n",
            "Iteration: 1980; Percent complete: 99.0%; Average loss: 0.0003\n",
            "Iteration: 1981; Percent complete: 99.1%; Average loss: 0.0003\n",
            "Iteration: 1982; Percent complete: 99.1%; Average loss: 0.0003\n",
            "Iteration: 1983; Percent complete: 99.2%; Average loss: 0.0003\n",
            "Iteration: 1984; Percent complete: 99.2%; Average loss: 0.0003\n",
            "Iteration: 1985; Percent complete: 99.2%; Average loss: 0.0003\n",
            "Iteration: 1986; Percent complete: 99.3%; Average loss: 0.0003\n",
            "Iteration: 1987; Percent complete: 99.4%; Average loss: 0.0003\n",
            "Iteration: 1988; Percent complete: 99.4%; Average loss: 0.0003\n",
            "Iteration: 1989; Percent complete: 99.5%; Average loss: 0.0003\n",
            "Iteration: 1990; Percent complete: 99.5%; Average loss: 0.0003\n",
            "Iteration: 1991; Percent complete: 99.6%; Average loss: 0.0003\n",
            "Iteration: 1992; Percent complete: 99.6%; Average loss: 0.0003\n",
            "Iteration: 1993; Percent complete: 99.7%; Average loss: 0.0003\n",
            "Iteration: 1994; Percent complete: 99.7%; Average loss: 0.0003\n",
            "Iteration: 1995; Percent complete: 99.8%; Average loss: 0.0003\n",
            "Iteration: 1996; Percent complete: 99.8%; Average loss: 0.0003\n",
            "Iteration: 1997; Percent complete: 99.9%; Average loss: 0.0003\n",
            "Iteration: 1998; Percent complete: 99.9%; Average loss: 0.0003\n",
            "Iteration: 1999; Percent complete: 100.0%; Average loss: 0.0003\n",
            "Iteration: 2000; Percent complete: 100.0%; Average loss: 0.0003\n",
            "Trained.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnmanbd-3znW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You know what's gonna happen now :)\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "searcher = GreedySearchDecoder(encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDyqP3Un4vmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractExpression(input):\n",
        "    var = input.split(' ')\n",
        "    x = int(var[0])\n",
        "    y = int(var[1])\n",
        "    expression = var[2]\n",
        "\n",
        "    return x, y, expression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiZ7f6Nj7DAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "\n",
        "ops = { \"+\": operator.add, \"-\": operator.sub, \"*\": operator.mul,  \"^\": operator.pow} \n",
        "\n",
        "def displayResult(userInput, print_result = False):\n",
        "\n",
        "    neuralNetOutput = test(encoder, decoder, searcher, vocab, userInput)\n",
        "    x, y, expression = extractExpression(neuralNetOutput)\n",
        "    \n",
        "    if print_result is True:\n",
        "        print(\"X :\", x)\n",
        "        print(\"Y :\", y)\n",
        "        print(\"Expression :\", expression)\n",
        "\n",
        "    if expression[0] == 'x':\n",
        "        ans = ops[expression[1]](x,y)  \n",
        "    else:\n",
        "        ans = ops[expression[1]](y,x)  \n",
        "    \n",
        "    return ans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WabzIyY07GGb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e2bd6306-ffab-467b-cac9-ecf867dd659e"
      },
      "source": [
        "displayResult('add 2 and 3', print_result = True)\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X : 2\n",
            "Y : 3\n",
            "Expression : x+y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLL_kbyM_5la",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ans_list = [\"Answers\"]\n",
        "for exp in test_data:\n",
        "    ans = displayResult(exp)\n",
        "    ans_list.append(ans)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bXSL65_ANzI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f452dc20-7b0b-4925-cc8a-0cd88edae269"
      },
      "source": [
        "print(ans_list)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Answers', 7744, 54, 3135, 480, 59, -1, 5162, 79, 1189, 421875, 140, 117649, 125, 65, 2601, 24389, 3744, 130, 3080, 664, 19, 3225, 975, 116, 190, 84, 28, 314432, 49, 1610, 95, 380, 493039, -38, 118, -17, -54, 1369, 62, 4941, 12167, -52, 140, 78, 88, -16, -22, 97336, 28, 784, 750, 49, 126, 43, 6724, 88, 1050, 143, 29791, -6, 816, 782, 62, 529, 225, 46656, 120, 2, 676, 1610, 140608, 9801, 2, 132, -2, 111, -51, 80, 7938, -9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXXkUvHtBAwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('test.csv', 'r') as read_obj, \\\n",
        "        open('testResults.csv', 'w', newline='') as write_obj:\n",
        "\n",
        "    csv_reader = csv.reader(read_obj)\n",
        "    csv_writer = csv.writer(write_obj)\n",
        "\n",
        "    for i, row in enumerate(csv_reader):\n",
        "        row.append(ans_list[i])\n",
        "        csv_writer.writerow(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_SggiTeDy1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}